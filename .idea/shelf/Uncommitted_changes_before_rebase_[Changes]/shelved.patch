Index: cbf_eval_FW.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\nsys.path.insert(1, os.path.abspath('.'))\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport numpy as np\nimport torch\nfrom qp_control.NNfuncgrad import CBF\nfrom dynamics.fixed_wing import FixedWing\nfrom qp_control.utils import Utils\n\n\nn_state = 9\nm_control = 4\n\nfault = int(input(\"Fault (1) or pre-fault (0):\"))\n\nfault_control_index = 1\n\ndt = 0.01\nN = 100000\n\nnominal_params = {\n    \"m\": 1000.0,\n    \"g\": 9.8,\n    \"Ixx\": 100,\n    \"Iyy\": 100,\n    \"Izz\": 1000,\n    \"Ixz\": 0.1,\n    \"S\": 25,\n    \"b\": 4,\n    \"bar_c\": 4,\n    \"rho\": 1.3,\n    \"Cd0\": 0.0434,\n    \"Cda\": 0.22,\n    \"Clb\": -0.13,\n    \"Clp\": -0.505,\n    \"Clr\": 0.252,\n    \"Clda\": 0.0855,\n    \"Cldr\": -0.0024,\n    \"Cm0\": 0.135,\n    \"Cma\": -1.50,\n    \"Cmq\": -38.2,\n    \"Cmde\": -0.992,\n    \"Cnb\": 0.0726,\n    \"Cnp\": -0.069,\n    \"Cnr\": -0.0946,\n    \"Cnda\": 0.7,\n    \"Cndr\": -0.0693,\n    \"Cyb\": -0.83,\n    \"Cyp\": 1,\n    \"Cyr\": 1,\n    \"Cydr\": 1,\n    \"Cz0\": 0.23,\n    \"Cza\": 4.58,\n    \"Czq\": 1,\n    \"Czde\": 1,\n    \"Cx0\": 1,\n    \"Cxq\": 1,\n    \"fault\": fault,}\n\nstate = torch.tensor([[100.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0]])\n\n\ndynamics = FixedWing(x=state, nominal_params=nominal_params, dt=dt, controller_dt=dt)\nutil = Utils(n_state=9, m_control=4, j_const=2, dyn=dynamics, dt=dt, params=nominal_params, fault=fault,\n                 fault_control_index=fault_control_index)\n\nsu, sl = dynamics.state_limits()\n\nif fault == 0:\n    cbf = CBF(dynamics, n_state=n_state, m_control=m_control,fault = 0, fault_control_index = 1)\n    cbf.load_state_dict(torch.load('./data/FW_cbf_NN_weights.pth'))\n    cbf.eval()\nelse:\n    cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault =0, fault_control_index = 1)\n    cbf.load_state_dict(torch.load('./data/FW_cbf_FT_weights.pth'))\n    cbf.eval()\n\nsafety_rate = 0.0\ncorrect_h = 0.0\n# for i in range(N):\n#     state = (su.clone().reshape(1,n_state) + sl.clone().reshape(1,n_state)) / 2 + torch.rand(1,n_state)\n#     h, _ = cbf.V_with_jacobian(state)\n#     safety_rate = safety_rate * (1-1 / N) + int(util.is_safe(state)) / N\n#     cor_h = int(util.is_safe(state) * h >= 0)\n#     correct_h = correct_h * (1 - 1 / N) +  cor_h / N\n\nstate = torch.tensor([]).reshape(0,n_state) #torch.zeros(N,n_state).reshape(N,n_state)\nfor j in range(N):\n    state_N = (su.clone() + sl.clone()) / 2 + 1 * torch.randn(1, n_state)\n    state = torch.vstack((state,state_N))\n\nstate = state.reshape(N,n_state,1)\n\nh, _  = cbf.V_with_jacobian(state)\n\nsafety_rate = torch.sum(util.is_safe(state)) / N\n\nun_safety_rate = torch.sum(torch.logical_not(util.is_safe(state))) / N\n\ncorrect_h_safe = torch.sum(util.is_safe(state).reshape(1, N) * (h >= 0).reshape(1, N)) / N\ncorrect_h_un_safe = torch.sum(util.is_unsafe(state).reshape(1, N) * (h < 0).reshape(1, N)) / N\n\nprint(safety_rate)\nprint(un_safety_rate)\nprint(correct_h_safe / safety_rate)\nprint(correct_h_un_safe / un_safety_rate)\n\n# import matplotlib.pyplot as plt\n\n# alpha_index = 1\n# # import pdb; pdb.set_trace()\n# state[:, 2:, :] = 0\n# plt.scatter(state[:, alpha_index, 0].detach().numpy(), state[:, 0, 0].detach().numpy(), c=util.is_safe(state).type(torch.float).squeeze().detach().numpy())\n# plt.show()\n# plt.scatter(state[:, alpha_index, 0].detach().numpy(), state[:, 0, 0].detach().numpy(), c=h.squeeze().detach().numpy())\n# plt.colorbar()\n\n# plt.show()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cbf_eval_FW.py b/cbf_eval_FW.py
--- a/cbf_eval_FW.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/cbf_eval_FW.py	(date 1666107636915)
@@ -1,129 +1,155 @@
 import os
 import sys
-sys.path.insert(1, os.path.abspath('.'))
-
 import matplotlib.pyplot as plt
-plt.style.use('seaborn-white')
 import numpy as np
 import torch
-from qp_control.NNfuncgrad import CBF
+from qp_control import config
+from qp_control.constraints_fw import constraints
+from qp_control.NNfuncgrad import CBF, NNController_new, alpha_param
 from dynamics.fixed_wing import FixedWing
 from qp_control.utils import Utils
 
+plt.style.use('seaborn-white')
+
+sys.path.insert(1, os.path.abspath('.'))
 
 n_state = 9
 m_control = 4
 
-fault = int(input("Fault (1) or pre-fault (0):"))
-
 fault_control_index = 1
 
 dt = 0.01
-N = 100000
+n_sample = 10000
+N1 = n_sample
+N2 = 10000
 
-nominal_params = {
-    "m": 1000.0,
-    "g": 9.8,
-    "Ixx": 100,
-    "Iyy": 100,
-    "Izz": 1000,
-    "Ixz": 0.1,
-    "S": 25,
-    "b": 4,
-    "bar_c": 4,
-    "rho": 1.3,
-    "Cd0": 0.0434,
-    "Cda": 0.22,
-    "Clb": -0.13,
-    "Clp": -0.505,
-    "Clr": 0.252,
-    "Clda": 0.0855,
-    "Cldr": -0.0024,
-    "Cm0": 0.135,
-    "Cma": -1.50,
-    "Cmq": -38.2,
-    "Cmde": -0.992,
-    "Cnb": 0.0726,
-    "Cnp": -0.069,
-    "Cnr": -0.0946,
-    "Cnda": 0.7,
-    "Cndr": -0.0693,
-    "Cyb": -0.83,
-    "Cyp": 1,
-    "Cyr": 1,
-    "Cydr": 1,
-    "Cz0": 0.23,
-    "Cza": 4.58,
-    "Czq": 1,
-    "Czde": 1,
-    "Cx0": 1,
-    "Cxq": 1,
-    "fault": fault,}
+nominal_params = config.FIXED_WING_PARAMS
+fault = nominal_params["fault"]
 
 state = torch.tensor([[100.0,
-                    0.0,
-                    0.0,
-                    0.0,
-                    0.0,
-                    0.0,
-                    0.0,
-                    0.0,
-                    0.0]])
+                       0.0,
+                       0.0,
+                       0.0,
+                       0.0,
+                       0.0,
+                       0.0,
+                       0.0,
+                       0.0]])
+
+goal = torch.tensor([[100.0,
+                      0.0,
+                      0.0,
+                      0.0,
+                      0.0,
+                      0.0,
+                      0.0,
+                      0.0,
+                      0.0]])
 
+goal = np.array(goal).reshape(1, n_state)
 
 dynamics = FixedWing(x=state, nominal_params=nominal_params, dt=dt, controller_dt=dt)
 util = Utils(n_state=9, m_control=4, j_const=2, dyn=dynamics, dt=dt, params=nominal_params, fault=fault,
-                 fault_control_index=fault_control_index)
+             fault_control_index=fault_control_index)
 
 su, sl = dynamics.state_limits()
 
+cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault=fault, fault_control_index=1)
+nn_controller = NNController_new(n_state=9, m_control=4)
+alpha = alpha_param(n_state=9)
+
 if fault == 0:
-    cbf = CBF(dynamics, n_state=n_state, m_control=m_control,fault = 0, fault_control_index = 1)
     cbf.load_state_dict(torch.load('./data/FW_cbf_NN_weights.pth'))
+    nn_controller.load_state_dict(torch.load('./data/FW_controller_NN_weights.pth'))
+    alpha.load_state_dict(torch.load('./data/FW_alpha_NN_weights.pth'))
     cbf.eval()
+    nn_controller.eval()
+    alpha.eval()
 else:
-    cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault =0, fault_control_index = 1)
     cbf.load_state_dict(torch.load('./data/FW_cbf_FT_weights.pth'))
+    nn_controller.load_state_dict(torch.load('./data/FW_controller_FT_weights.pth'))
+    alpha.load_state_dict(torch.load('./data/FW_alpha_FT_weights.pth'))
     cbf.eval()
+    nn_controller.eval()
+    alpha.eval()
+
+safe_m, safe_l = dynamics.safe_limits(su, sl)
 
+u_nominal = torch.zeros(N1 + N2, m_control)
+
+deriv_safe = 0.0
 safety_rate = 0.0
-correct_h = 0.0
-# for i in range(N):
-#     state = (su.clone().reshape(1,n_state) + sl.clone().reshape(1,n_state)) / 2 + torch.rand(1,n_state)
-#     h, _ = cbf.V_with_jacobian(state)
-#     safety_rate = safety_rate * (1-1 / N) + int(util.is_safe(state)) / N
-#     cor_h = int(util.is_safe(state) * h >= 0)
-#     correct_h = correct_h * (1 - 1 / N) +  cor_h / N
+un_safety_rate = 0.0
+correct_h_safe = 0.0
+correct_h_un_safe = 0.0
+
+iterations = 10
+
+for k in range(iterations):
+    # print(k)
+    state_bndr = util.x_bndr(safe_m, safe_l, n_sample)
+
+    state_bndr = state_bndr.reshape(N1, n_state)
+
+    state = state_bndr + 10 * torch.randn(N1, n_state)
+
+    for j in range(N2):
+        state_temp = (su.clone() + sl.clone()) / 2 + 1 * torch.randn(1, n_state)
+        state = torch.vstack((state, state_temp))
+
+    state = state.reshape(N1 + N2, n_state)
+
+    h, grad_h = cbf.V_with_jacobian(state)
+
+    fx = dynamics._f(state, params=nominal_params)
+
+    gx = dynamics._g(state, params=nominal_params)
+
+    # u_n = util.nominal_controller(state=state, goal=goal, u_n=u_nominal, dyn=dynamics, constraints=constraints)
+
+    # u_nominal = util.neural_controller(u_n, fx, gx, h, grad_h, fault_start=fault)
+
+    # u_nominal = u_n.reshape(N1+N2, m_control)
+
+    u = nn_controller(torch.tensor(state, dtype=torch.float32), torch.tensor(u_nominal, dtype=torch.float32))
+
+    dsdt = fx + torch.matmul(gx, u.reshape(N1 + N2, m_control, 1))
+
+    dsdt = torch.reshape(dsdt, (N1 + N2, n_state))
+
+    alpha_p = alpha(state)
 
-state = torch.tensor([]).reshape(0,n_state) #torch.zeros(N,n_state).reshape(N,n_state)
-for j in range(N):
-    state_N = (su.clone() + sl.clone()) / 2 + 1 * torch.randn(1, n_state)
-    state = torch.vstack((state,state_N))
+    dot_h = torch.matmul(grad_h.reshape(N1 + N2, 1, n_state),
+                         dsdt.reshape(N1 + N2, n_state, 1))
 
-state = state.reshape(N,n_state,1)
+    dot_h = dot_h.reshape(N1 + N2, 1)
 
-h, _  = cbf.V_with_jacobian(state)
+    deriv_cond = dot_h + alpha_p * h
 
-safety_rate = torch.sum(util.is_safe(state)) / N
+    deriv_safe += torch.sum((deriv_cond >= 0).reshape(N1+N2, 1) * util.is_safe(state).reshape(N1+N2, 1)) / (N1 + N2)
 
-un_safety_rate = torch.sum(torch.logical_not(util.is_safe(state))) / N
+    safety_rate += torch.sum(util.is_safe(state)) / (N1 + N2)
 
-correct_h_safe = torch.sum(util.is_safe(state).reshape(1, N) * (h >= 0).reshape(1, N)) / N
-correct_h_un_safe = torch.sum(util.is_unsafe(state).reshape(1, N) * (h < 0).reshape(1, N)) / N
+    un_safety_rate += torch.sum(util.is_unsafe(state)) / (N1 + N2)
 
-print(safety_rate)
-print(un_safety_rate)
+    correct_h_safe += torch.sum(util.is_safe(state).reshape(1, N1 + N2) * (h >= 0).reshape(1, N1 + N2)) / (N1 + N2)
+
+    correct_h_un_safe += torch.sum(util.is_unsafe(state).reshape(1, N1 + N2) * (h < 0).reshape(1, N1 + N2)) / (N1 + N2)
+
+print(safety_rate / iterations)
+
+print(un_safety_rate / iterations)
+
 print(correct_h_safe / safety_rate)
+
 print(correct_h_un_safe / un_safety_rate)
 
+print(deriv_safe / safety_rate)
+
+# print(N_safe)
 # import matplotlib.pyplot as plt
-
-# alpha_index = 1
-# # import pdb; pdb.set_trace()
-# state[:, 2:, :] = 0
-# plt.scatter(state[:, alpha_index, 0].detach().numpy(), state[:, 0, 0].detach().numpy(), c=util.is_safe(state).type(torch.float).squeeze().detach().numpy())
-# plt.show()
-# plt.scatter(state[:, alpha_index, 0].detach().numpy(), state[:, 0, 0].detach().numpy(), c=h.squeeze().detach().numpy())
-# plt.colorbar()
-
+# alpha_index = 1 # import pdb; pdb.set_trace() state[:, 2:, :] = 0 plt.scatter(state[:, alpha_index,
+# 0].detach().numpy(), state[:, 0, 0].detach().numpy(), c=util.is_safe(state).type(torch.float).squeeze().detach(
+# ).numpy()) plt.show() plt.scatter(state[:, alpha_index, 0].detach().numpy(), state[:, 0, 0].detach().numpy(),
+# c=h.squeeze().detach().numpy()) plt.colorbar()
 # plt.show()
Index: dynamics/fixed_wing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Define a dynamical system for a 3D quadrotor\"\"\"\nfrom typing import Tuple, List, Optional\n\nimport torch\nimport numpy as np\n\nfrom .control_affine_system_new import ControlAffineSystemNew\n# from .utils import grav, Scenario, ScenarioList\n# from neural_clbf.systems.utils import grav, Scenario, ScenarioList\n\n\nclass FixedWing(ControlAffineSystemNew):\n    \"\"\"\n    Represents a planar quadrotor.\n\n    The system has state\n\n        x = [px, py, pz, vx, vy, vz, phi, theta, psi]\n\n    representing the position, orientation, and velocities of the quadrotor, and it\n    has control inputs\n\n        u = [f, phi_dot, theta_dot, psi_dot]\n\n    The system is parameterized by\n        m: mass\n\n    NOTE: Z is defined as positive downwards\n    \"\"\"\n\n    # Number of states and controls\n    N_DIMS = 9\n    N_CONTROLS = 4\n\n    # State indices\n    \n    V = 0\n    ALPHA = 1\n    BETA = 2\n\n    PHI = 3\n    GAMMA = 4\n    PSI = 5\n\n    P = 6\n    Q = 7\n    R = 8\n\n\n    # Control indices\n    T = 0\n    DA = 1\n    DE = 2\n    DR = 3\n\n    # self.x = x\n    # self.params = params\n\n\n\n    def __init__(\n        self,\n        x: torch.Tensor,\n        nominal_params,\n        dt: float = 0.01,\n        controller_dt: Optional[float] = None,\n        # scenarios: Optional[ScenarioList] = None,\n        ):\n        self.x = x\n        self.params = nominal_params\n        \"\"\"\n        Initialize the quadrotor.\n\n        args:\n            nominal_params: a dictionary giving the parameter values for the system.\n                            Requires keys [\"m\"]\n            dt: the timestep to use for the simulation\n            controller_dt: the timestep for the LQR discretization. Defaults to dt\n        raises:\n            ValueError if nominal_params are not valid for this system\n        \"\"\"\n        super().__init__(x, nominal_params, dt, controller_dt)\n\n    def validate_params(self, params) -> bool:\n\n        # print(params)\n        \"\"\"Check if a given set of parameters is valid\n\n        args:\n            params: a dictionary giving the parameter values for the system.\n                    Requires keys [\"m\"]\n        returns:\n            True if parameters are valid, False otherwise\n        \"\"\"\n        valid = True\n        # Make sure all needed parameters were provided\n        valid = valid and \"m\" in params\n        valid = valid and \"g\" in params\n        valid = valid and \"Ixx\" in params\n        valid = valid and \"Iyy\" in params\n        valid = valid and \"Izz\" in params\n        valid = valid and \"Ixz\" in params\n        valid = valid and \"S\" in params\n        valid = valid and \"b\" in params\n        valid = valid and \"bar_c\" in params\n        valid = valid and \"rho\" in params\n        valid = valid and \"Cd0\" in params\n        valid = valid and \"Cda\" in params\n        valid = valid and \"Clb\" in params\n        valid = valid and \"Clda\" in params\n        valid = valid and \"Cldr\" in params\n        valid = valid and \"Clp\" in params\n        valid = valid and \"Clr\" in params\n        valid = valid and \"Cm0\" in params\n        valid = valid and \"Cmde\" in params\n        valid = valid and \"Cma\" in params\n        valid = valid and \"Cmq\" in params\n        valid = valid and \"Cnb\" in params\n        valid = valid and \"Cnda\" in params\n        valid = valid and \"Cndr\" in params\n        valid = valid and \"Cnr\" in params\n        valid = valid and \"Cndr\" in params\n        valid = valid and \"Cyb\" in params\n        valid = valid and \"Cyp\" in params\n        valid = valid and \"Cyr\" in params\n        valid = valid and \"Cydr\" in params\n        valid = valid and \"Cz0\" in params\n        valid = valid and \"Cza\" in params\n        valid = valid and \"Czq\" in params\n        valid = valid and \"Czde\" in params\n        valid = valid and \"Cx0\" in params\n        valid = valid and \"Cxq\" in params\n\n        # Make sure all parameters are physically valid\n        valid = valid and params[\"m\"] > 0\n        valid = valid and params[\"Ixx\"] > 0\n        valid = valid and params[\"Iyy\"] > 0\n        valid = valid and params[\"Izz\"] > 0\n        valid = valid and params[\"Ixz\"] > 0\n        valid = valid and params[\"S\"] > 0\n        valid = valid and params[\"b\"] > 0\n        valid = valid and params[\"bar_c\"] > 0\n        valid = valid and params[\"rho\"] > 0\n\n        return valid\n\n    @property\n    def n_dims(self) -> int:\n        return FixedWing.N_DIMS\n\n    @property\n    def angle_dims(self) -> List[int]:\n        return [FixedWing.PHI, FixedWing.GAMMA, FixedWing.PSI]\n\n    @property\n    def n_controls(self) -> int:\n        return FixedWing.N_CONTROLS\n\n    # @property\n    def state_limits(self) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Return a tuple (upper, lower) describing the expected range of states for this\n        system\n        \"\"\"\n        # define upper and lower limits based around the nominal equilibrium input\n        upper_limit = torch.ones(self.n_dims)\n        upper_limit[FixedWing.V] = 200.0\n        upper_limit[FixedWing.ALPHA] = np.pi / 3.0\n        upper_limit[FixedWing.BETA] = np.pi / 6.0\n        upper_limit[FixedWing.PHI] = np.pi / 3\n        upper_limit[FixedWing.GAMMA] = np.pi / 3\n        upper_limit[FixedWing.PSI] = np.pi / 3\n        upper_limit[FixedWing.P] = 4\n        upper_limit[FixedWing.Q] = 4\n        upper_limit[FixedWing.R] = 4\n\n        lower_limit = -1.0 * upper_limit\n        lower_limit[FixedWing.V] = 20.0\n\n        # lower_limit = torch.tensor(lower_limit)\n        # upper_limit = torch.tensor(upper_limit)\n\n        return (upper_limit, lower_limit)\n\n    # @property\n    def control_limits(self) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Return a tuple (upper, lower) describing the range of allowable control\n        limits for this system\n        \"\"\"\n        # define upper and lower limits based around the nominal equilibrium input\n        upper_limit = torch.tensor([1500, 2, 2, 2])\n        lower_limit = -1.0 * upper_limit\n        lower_limit[FixedWing.T] = 500\n\n        # lower_limit = torch.tensor(lower_limit)\n        # upper_limit = torch.tensor(upper_limit)\n\n        return (upper_limit, lower_limit)\n\n    def safe_mask(self, x):\n        \"\"\"Return the mask of x indicating safe regions for the obstacle task\n\n        args:\n            x: a tensor of points in the state space\n        \"\"\"\n        params = self.params\n        fault = params[\"fault\"]\n\n        safe_mask = torch.ones_like(x[:, 0], dtype=torch.bool)\n        if fault == 0:\n            safe_alpha = np.pi / 8.0\n            safe_alpha_l = - np.pi / 80.0\n            safe_beta = np.pi / 15\n        else:\n            safe_alpha = np.pi / 6.0\n            safe_alpha_l = - np.pi / 60.0\n            safe_beta = np.pi / 12\n        # safe_radius = 3\n\n        safe_mask = torch.logical_and(\n            x[:, FixedWing.ALPHA] <= safe_alpha, x[:, FixedWing.ALPHA] >= safe_alpha_l)\n        # safe_mask = torch.logical_and(safe_mask, x[:, FixedWing.BETA] <= safe_beta)\n        # safe_mask = torch.logical_and(safe_mask, x[:, FixedWing.BETA] >= -safe_beta)\n\n        return safe_mask\n\n    def safe_limits(self):\n        \"\"\"Return the mask of x indicating safe regions for the obstacle task\n\n        args:\n            x: a tensor of points in the state space\n        \"\"\"\n        fault = self.params[\"fault\"]\n        if fault == 0:\n            safe_alpha = np.pi / 8.0\n            safe_alpha_l = - np.pi / 80.0\n            safe_beta = np.pi / 15\n        else:\n            safe_alpha = np.pi / 6.0\n            safe_alpha_l = - np.pi / 60.0\n            safe_beta = np.pi / 12\n        # safe_radius = 3\n\n        safe_l = [safe_alpha_l, -safe_beta]\n        safe_m = [safe_alpha, safe_beta]\n        # safe_mask = torch.logical_and(safe_mask, x[:, FixedWing.BETA] <= safe_beta)\n        # safe_mask = torch.logical_and(safe_mask, x[:, FixedWing.BETA] >= -safe_beta)\n\n        return safe_m, safe_l\n\n    def unsafe_mask(self, x):\n        \"\"\"Return the mask of x indicating unsafe regions for the obstacle task\n\n        args:\n            x: a tensor of points in the state space\n        \"\"\"\n        params = self.params\n        fault = params[\"fault\"]\n\n        unsafe_mask = torch.zeros_like(x[:, 0], dtype=torch.bool)\n        if fault == 0:\n            unsafe_alpha = np.pi / 7.5\n            unsafe_alpha_l = - np.pi / 70.0\n            unsafe_beta = np.pi / 14\n        else:\n            unsafe_alpha = np.pi / 5.5\n            unsafe_alpha_l = - np.pi / 50.0\n            unsafe_beta = np.pi / 11\n\n\n        unsafe_mask = torch.logical_or(\n            x[:, FixedWing.ALPHA] >= unsafe_alpha, x[:, FixedWing.ALPHA] <= unsafe_alpha_l)\n        # unsafe_mask = torch.logical_or(unsafe_mask, x[:, FixedWing.BETA] >= unsafe_beta)\n        # unsafe_mask = torch.logical_or(unsafe_mask, x[:, FixedWing.BETA] <= -unsafe_beta)\n\n        return unsafe_mask\n\n    def goal_mask(self, x):\n        \"\"\"Return the mask of x indicating points in the goal set (within 0.2 m of the\n        goal).\n\n        args:\n            x: a tensor of points in the state space\n        \"\"\"\n        goal_mask = torch.ones_like(x[:, 0], dtype=torch.bool)\n\n        # Define the goal region as being near the goal\n        near_goal = torch.logical_and(x[:,FixedWing.V]<= 105, x[:,FixedWing.V] >= 95)\n        # near_goal = x.norm(dim=-1) <= 0.3\n        goal_mask.logical_and_(near_goal)\n\n        # The goal set has to be a subset of the safe set\n        goal_mask.logical_and_(self.safe_mask(x))\n\n        return goal_mask\n\n    def _f(self, x: torch.Tensor, params):\n        \"\"\"\n        Return the control-independent part of the control-affine dynamics.\n\n        args:\n            x: bs x self.n_dims tensor of state\n            params: a dictionary giving the parameter values for the system. If None,\n                    default to the nominal parameters used at initialization\n        returns:\n            f: bs x self.n_dims x 1 tensor\n        \"\"\"\n        # Extract batch size and set up a tensor for holding the result\n        batch_size = x.shape[0]\n        \n        m = params[\"m\"]\n        grav = params[\"g\"]\n        Ixx = params[\"Ixx\"]\n        Iyy = params[\"Iyy\"]\n        Izz = params[\"Izz\"]\n        Ixz = params[\"Ixz\"]\n        \n        S = params[\"S\"]\n        b = params[\"b\"]\n        bar_c = params[\"bar_c\"]\n        rho = params[\"rho\"]\n        \n        Cd0 = params[\"Cd0\"]\n        Cda = params[\"Cda\"]\n\n        Clb = params[\"Clb\"]\n        Clda = params[\"Clda\"]\n        Cldr = params[\"Cldr\"]\n        Clp = params[\"Clp\"]\n        Clr = params[\"Clr\"]\n\n        Cm0 = params[\"Cm0\"]\n        Cma = params[\"Cma\"]\n        Cmde = params[\"Cmde\"]\n        Cmq = params[\"Cmq\"]\n\n        Cnb = params[\"Cnb\"]\n        Cndr = params[\"Cndr\"]\n        Cnda = params[\"Cnda\"]\n        Cnp = params[\"Cnp\"]\n        Cnr = params[\"Cnr\"]\n\n        Cyb = params[\"Cyb\"]\n        Cydr = params[\"Cydr\"]\n        Cyp = params[\"Cyp\"]\n        Cyr = params[\"Cyr\"]\n\n        Cz0 = params[\"Cz0\"]\n        Cza = params[\"Cza\"]\n        Czde = params[\"Czde\"]\n        Czq = params[\"Czq\"]  \n\n        Cx0 = params[\"Cx0\"]\n        Cxq = params[\"Cxq\"]      \n\n\n\n\n        I_Gamma = Ixx * Izz - Ixz * Ixz\n\n        c1 = (Ixx - Iyy + Izz) * Ixz / I_Gamma\n        c2 = (Izz * Izz - Iyy * Izz + Ixz * Ixz) / I_Gamma\n        c3 = Izz / I_Gamma\n        c4 = Ixz / I_Gamma\n        c5 = (Izz - Ixx) / Iyy\n        c6 = Ixz / Iyy\n        c7 = 1.0 / Iyy\n        c8 = (Ixx * Ixx - Ixx * Iyy + Ixz * Ixz) / I_Gamma\n        c9 = Ixx / I_Gamma\n\n\n\n        # print(FixedWing.V)\n\n        V = x[:, FixedWing.V].reshape(batch_size,1)\n\n        # print(V)\n\n        alpha = x[:,FixedWing.ALPHA].reshape(batch_size,1)\n        beta = x[:,FixedWing.BETA].reshape(batch_size,1)\n        phi = x[:,FixedWing.PHI].reshape(batch_size,1)\n        gamma = x[:,FixedWing.GAMMA].reshape(batch_size,1)\n        psi = x[:,FixedWing.PSI].reshape(batch_size,1)\n        p = x[:,FixedWing.P].reshape(batch_size,1)\n        q = x[:,FixedWing.Q].reshape(batch_size,1)\n        r = x[:,FixedWing.R].reshape(batch_size,1)\n\n\n\n        \n        s_a = torch.sin(alpha).reshape(batch_size,1)\n        c_a = torch.cos(alpha).reshape(batch_size,1)\n\n        s_b = torch.sin(beta).reshape(batch_size,1)\n        c_b = torch.cos(beta).reshape(batch_size,1)\n        t_b = torch.tan(beta).reshape(batch_size,1)\n\n        s_g = torch.sin(gamma).reshape(batch_size,1)\n        c_g = torch.cos(gamma).reshape(batch_size,1)\n        t_g = torch.tan(gamma).reshape(batch_size,1)\n\n        s_p = torch.sin(phi).reshape(batch_size,1)\n        c_p = torch.cos(phi).reshape(batch_size,1)\n\n\n\n        bar_p = 1.0 / 2.0 * rho * rho * V * V * S\n\n        # D = Cd0 * bar_p\n\n        FX = (Cx0 + Cxq * (q * bar_c / 2 / V)) * bar_p\n\n        FY = - (Cyb * beta + b / 2 / V * (Cyp * p + Cyr * r)) * bar_p\n\n        FZ = (Cz0 + Cza * alpha + q * bar_c / 2 / V * Czq) * bar_p\n\n        # print(\"366\")\n\n        # print(V)\n\n        # print(FX)\n\n        Fx_t = FX.reshape(batch_size,1)\n        # Fx_t = Fx_t.detach().numpy()\n        # Fx_t = np.array(Fx_t).reshape(batch_size,1)\n        Fy_t = FY.reshape(batch_size,1)\n        # Fy_t = Fy_t.detach().numpy()\n        # Fy_t = np.array(Fy_t).reshape(batch_size,1)\n        Fz_t = FZ.reshape(batch_size,1)\n        # Fz_t = Fz_t.detach().numpy()\n        # Fz_t = np.array(Fz_t).reshape(batch_size,1)\n        # Fbf = torch.tensor([FX, FY, FZ])\n        # Fbf = np.array([[Fx_t], [Fy_t], [Fz_t]]).reshape(3,batch_size)\n        # Fbf = Fbf.detach().numpy()\n        # Fbf = np.array(Fbf).reshape(3,1)\n\n        Rw11 = c_a * c_b \n        # Rw11 = Rw11.detach().numpy()\n        Rw11 = Rw11.reshape(batch_size,1)\n        \n        Rw12 = s_b \n        # Rw12 = Rw12.detach().numpy()\n        Rw12 = Rw12.reshape(batch_size,1)\n        \n        Rw13 =  s_a * c_b\n        # Rw13 = Rw13.detach().numpy()\n        Rw13 = Rw13.reshape(batch_size,1)\n        \n        Rw21 = -c_a * s_b\n        # Rw21 = Rw21.detach().numpy()\n        Rw21 = Rw21.reshape(batch_size,1)\n        \n        Rw22 = c_b \n        # Rw22 = Rw22.detach().numpy()\n        Rw22 = Rw22.reshape(batch_size,1)\n        \n        Rw23 = -s_a * s_b\n        # Rw23 = Rw23.detach().numpy()\n        Rw23 = Rw23.reshape(batch_size,1)\n        \n        Rw31 = -s_a \n        # Rw31 = Rw31.detach().numpy()\n        Rw31 = Rw31.reshape(batch_size,1)\n        \n        Rw32 = torch.tensor([0.0]*batch_size).reshape(batch_size,1)\n        # Rw32 = Rw32.detach().numpy()\n        # Rw32 = np.array(Rw32).reshape(batch_size,1)\n        \n        Rw33 = c_a\n        # Rw33 = Rw33.detach().numpy()\n        Rw33 = Rw33.reshape(batch_size,1)\n\n        # Rw1 = np.array([Rw11, Rw12, Rw13]).reshape(batch_size,3)\n        # Rw2 = np.array([Rw21, Rw22, Rw23]).reshape(batch_size,3)\n        # Rw3 = np.array([Rw31, Rw32, Rw33]).reshape(batch_size,3)\n        # Rwb = np.array([Rw1, Rw2, Rw3]).reshape(3,batch_size,3)\n        # Rwb = torch.tensor([, , ])\n        # Rwb = Rwb.detach().numpy()\n        # Rwb = np.array(Rwb).reshape(3,3)\n\n        # Fwf = np.matmul(Rwb, Fbf) # .reshape(3, batch_size)\n\n\n        # print(Rwb)\n        # Fwf = np.array(Fwf, dtype = float).reshape(3,batch_size)\n\n\n        # print(Fwf)\n        # if batch_size == 64:\n        #     print(np.multiply(Rw11, Fx_t))\n\n        #     print(Fwf)\n\n        D = torch.multiply(Rw11, Fx_t) + torch.multiply(Rw12, Fy_t) + torch.multiply(Rw13, Fz_t)\n\n        Y = torch.multiply(Rw21, Fx_t) + torch.multiply(Rw32, Fy_t) + torch.multiply(Rw23, Fz_t)\n\n        L = torch.multiply(Rw31, Fx_t) + torch.multiply(Rw32, Fy_t) + torch.multiply(Rw33, Fz_t)\n\n        Fwx = - D\n        # Fwx = torch.tensor([- D]).reshape(batch_size,1)\n\n        # print(batch_size)\n\n        # print(Fwf)\n\n        # Fwy = torch.tensor([- Y]).reshape(batch_size,1)\n        # Fwz = torch.tensor([- L]).reshape(batch_size,1)\n        Fwy = - Y \n        Fwz = - L\n\n        # if batch_size == 64:\n        #     # print(Fwx)\n        #     print(FZ.shape)\n        #     print(s_g.shape)\n\n        # L = (Clb * beta + b / 2 / V * (Clp * p + Clr * r)) * bar_p\n\n        La = b * (Clb * beta + b / 2 / V * (Clp * p + Clr * r)) * bar_p\n\n        Ma = bar_c * (Cm0 + Cma * alpha + bar_c / 2 / V *(Cmq * q)) * bar_p\n\n        Na = b * (Cnb * beta + b / 2 / V *(Cnp * p + Cnr * r)) * bar_p\n\n        # print(Fwf)\n\n        \n        f = torch.zeros((batch_size, self.n_dims, 1))\n        f = f.type_as(x)\n\n\n        f[:, FixedWing.V] = - grav * s_g + Fwx / m\n\n        # f[:, FixedWing.V] = torch.matmul(-1.0*grav,torch.sin(gamma))\n        \n        # qwf = -1.0* torch.div(torch.matmul(grav,torch.matmul(torch.cos(gamma),torch.cos(phi))),V)\n\n        qwf = -1.0 * grav * c_g * c_p / V - 1.0 * Fwz / m / V\n\n        # if batch_size == 64:\n        #     print(V.shape)\n        #     print((-1.0 * torch.multiply(c_p,c_g)).shape)\n        #     print(torch.div(Fwz,V).shape)            \n\n        # f[:, FixedWing.ALPHA] = q - torch.matmul((torch.matmul(p, torch.cos(alpha))+torch.matmul(r,torch.sin(alpha))),torch.tan(beta)) -torch.div(qwf,torch.cos(beta))\n        f[:, FixedWing.ALPHA] = q - qwf / c_b - (p * c_a + r * s_a) * t_b\n        \n        # rwf = torch.div(torch.matmul(grav,torch.matmul(torch.cos(gamma),torch.sin(phi))),V) \n        rwf = grav * c_g * s_p / V + Fwy / m / V\n\n        # f[:, FixedWing.BETA] = rwf + torch.matmul(p,torch.sin(alpha))-torch.matmul(r,torch.cos(alpha))\n        f[:, FixedWing.BETA] = rwf + p * s_a - r * c_a\n\n        # pw = torch.matmul(p,torch.matmul(torch.cos(alpha),torch.cos(beta))) + torch.matmul(q-f[:, FixedWing.ALPHA],torch.sin(beta)) + torch.matmul(r,torch.matmul(torch.sin(alpha),torch.cos(beta)))\n        pw = p * c_a * c_b + (q-f[:,FixedWing.ALPHA]) * s_b + r * s_a * c_b\n        \n        # f[:,FixedWing.PHI] = pw + torch.matmul(torch.matmul(qwf,sin(phi))+torch.matmul(rwf,torch.cos(phi)),torch.tan(gamma))\n        f[:,FixedWing.PHI] = pw + (qwf * s_p + rwf * c_p) * t_g\n\n        f[:,FixedWing.GAMMA] = qwf * c_g - rwf * s_g\n        \n        f[:,FixedWing.PSI] = (qwf * s_p + rwf * c_p) / c_g\n\n        f[:,FixedWing.P] = c1 * p * q - c2 * q * r +  c3 * La + c4 * Na\n\n        f[:, FixedWing.Q] = c5 * p * r - c6* (p * p - r * r) + c7 * Ma\n\n        f[:, FixedWing.R] = c8 * p * q - c1 * q * r + c4 * La + c9 * Na\n\n        \n        return f\n\n    def _g(self, x: torch.Tensor, params):\n        \"\"\"\n        Return the control-independent part of the control-affine dynamics.\n\n        args:\n            x: bs x self.n_dims tensor of state\n            params: a dictionary giving the parameter values for the system. If None,\n                    default to the nominal parameters used at initialization\n        returns:\n            g: bs x self.n_dims x self.n_controls tensor\n        \"\"\"\n        # Extract batch size and set up a tensor for holding the result\n        # print(x.shape)\n        # x = torch.tensor(x)\n        batch_size = x.shape[0]\n        g = torch.zeros((batch_size, self.n_dims, self.n_controls))\n        g = g.type_as(x)\n\n        # Extract the needed parameters\n        m = params[\"m\"]\n        grav = params[\"g\"]\n        Ixx = params[\"Ixx\"]\n        Iyy = params[\"Iyy\"]\n        Izz = params[\"Izz\"]\n        Ixz = params[\"Ixz\"]\n        \n        S = params[\"S\"]\n        b = params[\"b\"]\n        bar_c = params[\"bar_c\"]\n        rho = params[\"rho\"]\n        \n        Cd0 = params[\"Cd0\"]\n        Cda = params[\"Cda\"]\n\n        Clb = params[\"Clb\"]\n        Clda = params[\"Clda\"]\n        Cldr = params[\"Cldr\"]\n        Clp = params[\"Clp\"]\n        Clr = params[\"Clr\"]\n\n        Cm0 = params[\"Cm0\"]\n        Cma = params[\"Cma\"]\n        Cmde = params[\"Cmde\"]\n        Cmq = params[\"Cmq\"]\n\n        Cnb = params[\"Cnb\"]\n        Cndr = params[\"Cndr\"]\n        Cnda = params[\"Cnda\"]\n        Cnp = params[\"Cnp\"]\n        Cnr = params[\"Cnr\"]\n\n        Cyb = params[\"Cyb\"]\n        Cydr = params[\"Cydr\"]\n        Cyp = params[\"Cyp\"]\n        Cyr = params[\"Cyr\"]\n\n        Cz0 = params[\"Cz0\"]\n        Cza = params[\"Cza\"]\n        Czde = params[\"Czde\"]\n        Czq = params[\"Czq\"]  \n\n        Cx0 = params[\"Cx0\"]\n        Cxq = params[\"Cxq\"]\n\n\n\n        I_Gamma = Ixx * Izz - Ixz * Ixz\n\n        c1 = (Ixx - Iyy + Izz) * Ixz / I_Gamma\n        c2 = (Izz * Izz - Iyy * Izz + Ixz * Ixz) / I_Gamma\n        c3 = Izz / I_Gamma\n        c4 = Ixz / I_Gamma\n        c5 = (Izz - Ixx) / Iyy\n        c6 = Ixz / Iyy\n        c7 = 1.0 / Iyy\n        c8 = (Ixx * Ixx - Ixx * Iyy + Ixz * Ixz) / I_Gamma\n        c9 = Ixx / I_Gamma\n\n        V = x[:, FixedWing.V].reshape(batch_size)\n        alpha = x[:,FixedWing.ALPHA].reshape(batch_size)\n        beta = x[:,FixedWing.BETA].reshape(batch_size)\n        phi = x[:,FixedWing.PHI].reshape(batch_size)\n        gamma = x[:,FixedWing.GAMMA].reshape(batch_size)\n        psi = x[:,FixedWing.PSI].reshape(batch_size)\n        p = x[:,FixedWing.P].reshape(batch_size)\n        q = x[:,FixedWing.Q].reshape(batch_size)\n        r = x[:,FixedWing.R].reshape(batch_size)\n\n\n\n        \n        s_a = torch.sin(alpha).reshape(batch_size)\n        c_a = torch.cos(alpha).reshape(batch_size)\n\n        s_b = torch.sin(beta).reshape(batch_size)\n        c_b = torch.cos(beta).reshape(batch_size)\n        t_b = torch.tan(beta).reshape(batch_size)\n\n        s_g = torch.sin(gamma).reshape(batch_size)\n        c_g = torch.cos(gamma).reshape(batch_size)\n        t_g = torch.tan(gamma).reshape(batch_size)\n\n        s_p = torch.sin(phi).reshape(batch_size)\n        c_p = torch.cos(phi).reshape(batch_size)\n\n\n\n        bar_p = 1.0 / 2.0 * rho * rho * V * V * S\n\n        Rw11 = c_a * c_b \n        # Rw11 = Rw11.detach().numpy()\n        Rw11 = Rw11.reshape(batch_size)\n        \n        Rw12 = s_b \n        # Rw12 = Rw12.detach().numpy()\n        Rw12 = Rw12.reshape(batch_size)\n        \n        Rw13 =  s_a * c_b\n        # Rw13 = Rw13.detach().numpy()\n        Rw13 = Rw13.reshape(batch_size)\n        \n        Rw21 = -c_a * s_b\n        # Rw21 = Rw21.detach().numpy()\n        Rw21 = Rw21.reshape(batch_size)\n        \n        Rw22 = c_b \n        # Rw22 = Rw22.detach().numpy()\n        Rw22 = Rw22.reshape(batch_size)\n        \n        Rw23 = -s_a * s_b\n        # Rw23 = Rw23.detach().numpy()\n        Rw23 = Rw23.reshape(batch_size)\n        \n        Rw31 = -s_a \n        # Rw31 = Rw31.detach().numpy()\n        Rw31 = Rw31.reshape(batch_size)\n        \n        Rw32 = torch.tensor([0.0]*batch_size).reshape(batch_size)\n        # Rw32 = Rw32.detach().numpy()\n        # Rw32 = np.array(Rw32).reshape(batch_size,1)\n        \n        Rw33 = c_a\n        # Rw33 = Rw33.detach().numpy()\n        Rw33 = Rw33.reshape(batch_size)\n\n        # D = Cd0 * bar_p\n\n\n        FXda = torch.tensor([0.0]*batch_size).reshape(batch_size)\n        FXdr = FXda\n        FXde = FXda\n\n        FYda = FXda\n        FYdr = - (Cydr) * bar_p\n        FYde = FXda\n\n        FZda = FXda\n        FZdr = FXda\n        FZde = (Czde) * bar_p\n\n        # Fda = torch.tensor([FXda, FYda, FZda])\n        # Fda = Fda.detach().numpy()\n        # Fbfda = np.array(Fda).reshape(3,1)\n        \n        # Fdr = torch.tensor([FXdr, FYdr, FZdr])\n        # Fdr = Fdr.detach().numpy()\n        # Fbfdr = np.array(Fdr).reshape(3,1)\n        \n        # Fde = torch.tensor([FXde, FYde, FZde])\n        # Fde = Fde.detach().numpy()\n        # Fbfde = np.array(Fde).reshape(3,1)\n\n        # Rwb = torch.tensor([[c_a * c_b, s_b , s_a * c_b], [-c_a * s_b, c_b , -s_a * s_b], [-s_a, 0.0, c_a]])\n        # Rwb = Rwb.detach().numpy()\n\n        # Rwb = np.array(Rwb).reshape(3,3)\n\n\n\n        # Fwfda = np.dot(Rwb, Fbfda)\n        # Fwfdr = np.dot(Rwb, Fbfdr)\n        # Fwfde = np.dot(Rwb, Fbfde)\n\n        Dda = torch.multiply(Rw11, FXda) + torch.multiply(Rw12, FYda) + torch.multiply(Rw13, FZda)\n        Dde = torch.multiply(Rw11, FXde) + torch.multiply(Rw12, FYde) + torch.multiply(Rw13, FZde)\n        Ddr = torch.multiply(Rw11, FXdr) + torch.multiply(Rw12, FYdr) + torch.multiply(Rw13, FZdr)        \n\n        # Dda = Fwfda[0]\n        # Dde = Fwfde[0]\n        # Ddr = Fwfdr[0]\n\n        # Yda = Fwfda[1]\n        # Yde = Fwfde[1]\n        # Ydr = Fwfdr[1]\n        Yda = torch.multiply(Rw21, FXda) + torch.multiply(Rw22, FYda) + torch.multiply(Rw23, FZda)\n        Yde = torch.multiply(Rw21, FXde) + torch.multiply(Rw22, FYde) + torch.multiply(Rw23, FZde)\n        Ydr = torch.multiply(Rw21, FXdr) + torch.multiply(Rw22, FYdr) + torch.multiply(Rw23, FZdr)\n\n        # Lda = Fwfda[2]\n        # Lde = Fwfde[2]\n        # Ldr = Fwfdr[2]\n\n        Lda = torch.multiply(Rw31, FXda) + torch.multiply(Rw32, FYda) + torch.multiply(Rw33, FZda)\n        Lde = torch.multiply(Rw31, FXde) + torch.multiply(Rw32, FYde) + torch.multiply(Rw33, FZde)\n        Ldr = torch.multiply(Rw31, FXdr) + torch.multiply(Rw32, FYdr) + torch.multiply(Rw33, FZdr)\n\n        # Fwxda = torch.tensor([- Dda])\n        # Fwxdr = torch.tensor([- Ddr])\n        # Fwxde = torch.tensor([- Dde])\n        # Fwyda = torch.tensor([- Yda])\n        # Fwydr = torch.tensor([- Ydr])\n        # Fwyde = torch.tensor([- Yde])\n        # Fwzda = torch.tensor([- Lda])\n        # Fwzde = torch.tensor([- Lde])\n        # Fwzdr = torch.tensor([- Ldr])\n\n        Fwxda = - Dda\n        Fwxdr = - Ddr\n        Fwxde = - Dde\n        Fwyda = - Yda\n        Fwydr = - Ydr\n        Fwyde = - Yde\n        Fwzda = - Lda\n        Fwzde = - Lde\n        Fwzdr = - Ldr\n\n\n\n        Lada = b * (Clda) * bar_p\n        Ladr = b * (Clr) * bar_p\n        Lade = torch.tensor([0.0]*batch_size).reshape(batch_size)\n\n        Made = bar_c * (Cmde) * bar_p\n        Mada = Lade\n        Madr = Lade\n\n        Nada = b * (Cnda) * bar_p\n        Nadr = b * (Cndr) * bar_p\n        Nade = Lade\n\n        # if batch_size == 64:\n        #     print((c_a*c_b).shape)\n        #     print(g[:, FixedWing.V, FixedWing.T].shape)\n\n        g[:, FixedWing.V,FixedWing.T] = (c_a * c_b / m)\n\n        g[:, FixedWing.V,FixedWing.DA] =  Fwxda / m\n\n        g[:, FixedWing.V,FixedWing.DE] =  Fwxde / m\n\n        g[:, FixedWing.V,FixedWing.DR] =  Fwxdr / m\n\n\n\n        g[:, FixedWing.ALPHA, FixedWing.T] =  - s_a / m / V / c_b\n        g[:, FixedWing.ALPHA, FixedWing.DR] =  Fwzdr/ m / V / c_b\n        g[:, FixedWing.ALPHA, FixedWing.DA] =  Fwzda/ m / V / c_b\n        g[:, FixedWing.ALPHA, FixedWing.DE] =  Fwzde/ m / V / c_b\n\n        g[:, FixedWing.BETA, FixedWing.T] = -c_a * s_b / m / V\n        g[:, FixedWing.BETA, FixedWing.DA] =  Fwyda / m / V\n        g[:, FixedWing.BETA, FixedWing.DE] =  Fwyde / m / V\n        g[:, FixedWing.BETA, FixedWing.DR] =  Fwydr / m / V\n\n        g[:, FixedWing.PHI,FixedWing.T] = s_p / m / V * s_a * t_g + c_p * t_g / m / V * (-c_a * s_b)\n        g[:, FixedWing.PHI, FixedWing.DA] = -s_p / m / V *Fwzda * t_g + c_p * t_g / m / V * Fwyda\n        g[:, FixedWing.PHI, FixedWing.DR] = -s_p / m / V *Fwzdr * t_g + c_p * t_g / m / V * Fwyde\n        g[:, FixedWing.PHI, FixedWing.DE] = -s_p / m / V *Fwzdr * t_g + c_p * t_g / m / V * Fwyde\n\n        g[:, FixedWing.GAMMA, FixedWing.T] = - c_p / m / V * c_a * c_b  - s_p / m / V * (-c_a * s_b)\n        g[:, FixedWing.GAMMA, FixedWing.DA] = -c_p / m / V * Fwzda - s_p / m / V * Fwyda\n        g[:, FixedWing.GAMMA, FixedWing.DE] = -c_p / m / V * Fwzde - s_p / m / V * Fwyde\n        # print(Fwzdr)\n        g[:, FixedWing.GAMMA, FixedWing.DR] = -c_p * Fwzdr / m / V  - s_p * Fwydr / m / V \n\n        g[:, FixedWing.PSI, FixedWing.T] = - s_p / m / V * c_a * c_b / c_g  + c_p / m / V * (-c_a * s_b) / c_g\n        g[:, FixedWing.PSI, FixedWing.DA] = -s_p / m / V * Fwzda / c_g + c_p / m / V * Fwyda / c_g\n        g[:, FixedWing.PSI, FixedWing.DE] = -s_p / m / V * Fwzde / c_g + c_p / m / V * Fwyde / c_g\n        g[:, FixedWing.PSI, FixedWing.DR] = -s_p / m / V * Fwzdr / c_g + c_p / m / V * Fwydr / c_g\n\n        g[:, FixedWing.P, FixedWing.DA] = c3 * Lada + c4 * Nada\n        g[:, FixedWing.P, FixedWing.DE] = c3 * Lade + c4 * Nade\n        g[:, FixedWing.P, FixedWing.DR] = c3 * Ladr + c4 * Nadr\n\n        g[:, FixedWing.Q, FixedWing.DA] = c7 * Mada\n        g[:, FixedWing.Q, FixedWing.DE] = c7 * Made\n        g[:, FixedWing.Q, FixedWing.DR] = c7 * Madr\n\n        g[:, FixedWing.R, FixedWing.DA] = c4 * Lada + c9 * Nada\n        g[:, FixedWing.R, FixedWing.DE] = c4 * Lade + c9 * Nade\n        g[:, FixedWing.R, FixedWing.DR] = c4 * Ladr + c9 * Nadr\n\n        # print(g)\n\n\n\n\n        # Derivatives of all orientations are control variables\n        # g[:, FixedWing.PHI :, FixedWing.PHI_DOT :] = torch.eye(self.n_controls - 1)\n\n        return g\n\n    def u_in(self):\n        x = self.x\n        params = self.params\n        grav = params[\"g\"]\n        m = params[\"m\"]\n\n        rho = params[\"rho\"]\n        S = params[\"S\"]\n        bar_c = params[\"bar_c\"]\n        b = params[\"b\"]\n\n        CCd0 = params[\"Cd0\"]\n        Cda = params[\"Cda\"]\n\n        Clb = params[\"Clb\"]\n        Clda = params[\"Clda\"]\n        Cldr = params[\"Cldr\"]\n        Clp = params[\"Clp\"]\n        Clr = params[\"Clr\"]\n\n        Cm0 = params[\"Cm0\"]\n        Cma = params[\"Cma\"]\n        Cmde = params[\"Cmde\"]\n        Cmq = params[\"Cmq\"]\n\n        Cnb = params[\"Cnb\"]\n        Cndr = params[\"Cndr\"]\n        Cnda = params[\"Cnda\"]\n        Cnp = params[\"Cnp\"]\n        Cnr = params[\"Cnr\"]\n\n        Cyb = params[\"Cyb\"]\n        Cydr = params[\"Cydr\"]\n        Cyp = params[\"Cyp\"]\n        Cyr = params[\"Cyr\"]\n\n        Cz0 = params[\"Cz0\"]\n        Cza = params[\"Cza\"]\n        Czde = params[\"Czde\"]\n        Czq = params[\"Czq\"]  \n\n        Cx0 = params[\"Cx0\"]\n        Cxq = params[\"Cxq\"]\n\n\n        # print(x)\n        # x = np.array(x)\n        # x = torch.from_numpy(x)\n\n        # V = x[:,FixedWing.V]\n        V = x[:,FixedWing.V]\n        alpha = x[:,FixedWing.ALPHA]\n        beta = x[:,FixedWing.BETA]\n        phi = x[:,FixedWing.PHI]\n        gamma = x[:,FixedWing.GAMMA]\n        psi = x[:,FixedWing.PSI]\n        p = x[:,FixedWing.P]\n        q = x[:,FixedWing.Q]\n        r = x[:,FixedWing.R]\n\n        bar_p = 1.0 / 2.0 * rho * rho * V * V * S\n\n\n\n        \n        s_a = torch.sin(alpha)\n        c_a = torch.cos(alpha)\n\n        s_b = torch.sin(beta)\n        c_b = torch.cos(beta)\n        t_b = torch.tan(beta)\n\n        s_g = torch.sin(gamma)\n        c_g = torch.cos(gamma)\n        t_g = torch.tan(gamma)\n\n        s_p = torch.sin(phi)\n        c_p = torch.cos(phi)\n\n        u_eq = torch.zeros((1, self.n_controls))\n        # print(u_eq)\n\n        u_eq[0, FixedWing.DE] =   (- Cm0 - Cma * alpha) / Cmde\n        de = u_eq[0, FixedWing.DE]\n\n        det = Clda * Cndr - Cldr * Cnda\n\n\n        u_eq[0, FixedWing.DA] = - Clb * beta * Cndr / det + Cnb * beta * Cldr / det\n        da = u_eq[0, FixedWing.DA]\n\n        u_eq[0, FixedWing.DR] =  Clb * beta * Cnda / det - Cnb * beta * Clda / det\n        dr = u_eq[0, FixedWing.DR]\n\n        FX = (Cx0 + Cxq * (q * bar_c / 2 / V)) * bar_p\n\n        FY = - (Cyb * beta + b / 2 / V * (Cyp * p + Cyr * r) + Cydr * dr) * bar_p\n\n        FZ = (Cz0 + Cza * alpha + q * bar_c / 2 / V * Czq + Czde * de) * bar_p\n\n        # print(Cnb * beta + Cnda * da + Cndr * dr)\n\n        # print(\"366\")\n\n        # print(V)\n\n        Fbf = torch.tensor([FX, FY, FZ])\n\n        Fbf = Fbf.detach().numpy()\n        Fbf = np.array(Fbf).reshape(3,1)\n\n        Rwb = torch.tensor([[c_a * c_b, s_b , s_a * c_b], [-c_a * s_b, c_b , -s_a * s_b], [-s_a, 0.0, c_a]])\n        Rwb = Rwb.detach().numpy()\n        Rwb = np.array(Rwb).reshape(3,3)\n\n        Fwf = np.dot(Rwb, Fbf)\n\n        D = Fwf[0]\n\n        print(D)\n\n\n        u_eq[:, FixedWing.T] =  m * grav * s_g + D / c_a / c_b\n\n\n        return u_eq\n\n    @property\n    def u_eq(self):\n        x = self.x\n        params = self.params\n        grav = params[\"g\"]\n        m = params[\"m\"]\n\n        rho = params[\"rho\"]\n        S = params[\"S\"]\n        bar_c = params[\"bar_c\"]\n        b = params[\"b\"]\n\n        CCd0 = params[\"Cd0\"]\n        Cda = params[\"Cda\"]\n\n        Clb = params[\"Clb\"]\n        Clda = params[\"Clda\"]\n        Cldr = params[\"Cldr\"]\n        Clp = params[\"Clp\"]\n        Clr = params[\"Clr\"]\n\n        Cm0 = params[\"Cm0\"]\n        Cma = params[\"Cma\"]\n        Cmde = params[\"Cmde\"]\n        Cmq = params[\"Cmq\"]\n\n        Cnb = params[\"Cnb\"]\n        Cndr = params[\"Cndr\"]\n        Cnda = params[\"Cnda\"]\n        Cnp = params[\"Cnp\"]\n        Cnr = params[\"Cnr\"]\n\n        Cyb = params[\"Cyb\"]\n        Cydr = params[\"Cydr\"]\n        Cyp = params[\"Cyp\"]\n        Cyr = params[\"Cyr\"]\n\n        Cz0 = params[\"Cz0\"]\n        Cza = params[\"Cza\"]\n        Czde = params[\"Czde\"]\n        Czq = params[\"Czq\"]  \n\n        Cx0 = params[\"Cx0\"]\n        Cxq = params[\"Cxq\"]\n\n\n        # print(x)\n        # x = np.array(x)\n        # x = torch.from_numpy(x)\n\n        # V = x[:,FixedWing.V]\n        V = x[:,FixedWing.V]\n        alpha = x[:,FixedWing.ALPHA]\n        beta = x[:,FixedWing.BETA]\n        phi = x[:,FixedWing.PHI]\n        gamma = x[:,FixedWing.GAMMA]\n        psi = x[:,FixedWing.PSI]\n        p = x[:,FixedWing.P]\n        q = x[:,FixedWing.Q]\n        r = x[:,FixedWing.R]\n\n        bar_p = 1.0 / 2.0 * rho * rho * V * V * S\n\n\n\n        \n        s_a = torch.sin(alpha)\n        c_a = torch.cos(alpha)\n\n        s_b = torch.sin(beta)\n        c_b = torch.cos(beta)\n        t_b = torch.tan(beta)\n\n        s_g = torch.sin(gamma)\n        c_g = torch.cos(gamma)\n        t_g = torch.tan(gamma)\n\n        s_p = torch.sin(phi)\n        c_p = torch.cos(phi)\n\n        FX = (Cx0 + Cxq * (q * bar_c / 2 / V)) * bar_p\n\n        FY = - (Cyb * beta + b / 2 / V * (Cyp * p + Cyr * r)) * bar_p\n\n        FZ = (Cz0 + Cza * alpha + q * bar_c / 2 / V * Czq) * bar_p\n\n        # print(\"366\")\n\n        # print(V)\n\n        Fbf = torch.tensor([FX, FY, FZ])\n\n        Fbf = Fbf.detach().numpy()\n        Fbf = np.array(Fbf).reshape(3,1)\n\n        Rwb = torch.tensor([[c_a * c_b, s_b , s_a * c_b], [-c_a * s_b, c_b , -s_a * s_b], [-s_a, 0.0, c_a]])\n        Rwb = Rwb.detach().numpy()\n        Rwb = np.array(Rwb).reshape(3,3)\n\n        Fwf = np.dot(Rwb, Fbf)\n\n        D = Fwf[0]\n\n        Y = Fwf[1]\n\n        L = Fwf[2]\n\n        Fwx = torch.tensor([- D])\n        Fwy = torch.tensor([- Y])\n        Fwz = torch.tensor([- L])\n\n        u_eq = torch.zeros((1, self.n_controls))\n        # print(u_eq)\n        u_eq[:, FixedWing.T] =  grav * s_g * m + D\n        u_eq[0, FixedWing.DE] =   (- Cm0 - Cma * alpha) / Cmde\n        det = Clda * Cndr - Cldr * Cnda\n        u_eq[0, FixedWing.DA] = - Clb * beta * Cndr / det + Cnb * beta * Cldr / det\n        u_eq[0, FixedWing.DR] =  Clb * beta * Cnda / det - Cnb * beta * Clda / det\n\n        return u_eq\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dynamics/fixed_wing.py b/dynamics/fixed_wing.py
--- a/dynamics/fixed_wing.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/dynamics/fixed_wing.py	(date 1666028441931)
@@ -225,13 +225,18 @@
 
         return safe_mask
 
-    def safe_limits(self):
+    def safe_limits(self, su, sl):
         """Return the mask of x indicating safe regions for the obstacle task
 
         args:
             x: a tensor of points in the state space
         """
         fault = self.params["fault"]
+
+        # safe_m, safe_l = self.safe_limits()
+        safe_m = su
+        safe_l = sl
+
         if fault == 0:
             safe_alpha = np.pi / 8.0
             safe_alpha_l = - np.pi / 80.0
@@ -242,8 +247,10 @@
             safe_beta = np.pi / 12
         # safe_radius = 3
 
-        safe_l = [safe_alpha_l, -safe_beta]
-        safe_m = [safe_alpha, safe_beta]
+        safe_l[FixedWing.ALPHA] = safe_alpha_l
+        safe_l[FixedWing.BETA] = -safe_beta
+        safe_m[FixedWing.ALPHA] = safe_alpha
+        safe_m[FixedWing.BETA] = safe_beta
         # safe_mask = torch.logical_and(safe_mask, x[:, FixedWing.BETA] <= safe_beta)
         # safe_mask = torch.logical_and(safe_mask, x[:, FixedWing.BETA] >= -safe_beta)
 
Index: qp_control/trainer_new.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport math\nimport scipy\nfrom torch import nn\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nfrom qpsolvers import solve_qp\nfrom osqp import OSQP\nfrom scipy.sparse import identity\nfrom scipy.sparse import vstack, csr_matrix, csc_matrix\n\nfrom qp_control.FxTS_GF import FxTS_Momentum \n\nfrom pytictoc import TicToc\ntorch.autograd.set_detect_anomaly(True)\n\nt = TicToc() \n\n\nclass Trainer(object):\n\n    def __init__(self, \n                 controller, \n                 cbf, \n                 alpha,\n                 dataset,\n                 dyn, \n                 n_pos,\n                 params, \n                 n_state,\n                 m_control,\n                 j_const = 1,\n                 dt=0.05, \n                 safe_alpha=0.3, \n                 dang_alpha=0.4, \n                 action_loss_weight=0.1,\n                 gpu_id=-1,\n                 lr_decay_stepsize=-1,\n                 fault = 0,\n                 fault_control_index = -1):\n\n        self.params = params\n        self.n_state = n_state\n        self.m_control = m_control\n        self.j_const = j_const\n        self.controller = controller\n        self.dyn = dyn\n        self.cbf = cbf\n        self.alpha = alpha\n        self.dataset = dataset\n        self.fault = fault\n        self.fault_control_index = fault_control_index\n        \n        self.controller_optimizer = torch.optim.Adam(\n            self.controller.parameters(), lr=5e-4, weight_decay=1e-5)\n        self.cbf_optimizer = torch.optim.Adam(\n            self.cbf.parameters(), lr=1e-4, weight_decay=1e-5)\n        self.alpha_optimizer = torch.optim.Adam(\n            self.alpha.parameters(),lr = 1e-4, weight_decay = 1e-5)\n        # self.controller_optimizer = FxTS_Momentum(\n        #     self.controller.parameters(), lr=5e-4,momentum = 0.2)\n        # self.cbf_optimizer = FxTS_Momentum(\n        #     self.cbf.parameters(), lr=1e-4,momentum = 0.2)\n        # self.alpha_optimizer = FxTS_Momentum(\n        #     self.alpha.parameters(), lr=5e-4,momentum = 0.2)\n\n        self.n_pos = n_pos\n        self.dt = dt\n        self.safe_alpha = safe_alpha\n        self.dang_alpha = dang_alpha\n        self.action_loss_weight = action_loss_weight\n        # if gpu_id >=0, use gpu in training\n        self.gpu_id = gpu_id\n\n        # the learning rate is decayed when self.train_cbf_and_controller is called\n        # lr_decay_stepsize times\n        self.lr_decay_stepsize = lr_decay_stepsize\n        if lr_decay_stepsize >= 0:\n            self.cbf_lr_scheduler = torch.optim.lr_scheduler.StepLR(\n                self.cbf_optimizer, step_size=lr_decay_stepsize, gamma=0.5)\n            self.controller_lr_scheduler = torch.optim.lr_scheduler.StepLR(\n                self.controller_optimizer, step_size=lr_decay_stepsize, gamma=0.5)\n\n\n    def train_cbf(self, batch_size=256, opt_iter=500, eps=0.1):\n\n\n        loss_np = 0.0\n        acc_np = np.zeros((5,), dtype=np.float32)\n        \n        for i in range(opt_iter):\n            grad_h, state, u, u_nominal, state_next = self.dataset.sample_data(batch_size)\n            u_nominal = torch.from_numpy(u_nominal)\n\n            if self.gpu_id >= 0:\n                grad_h = grad_h.cuda(self.gpu_id)\n                state = state.cuda(self.gpu_id)\n                u = u.cuda(self.gpu_id)\n                u_nominal = u_nominal.cuda(self.gpu_id)\n                state_next = state_next.cuda(self.gpu_id)\n\n            safe_mask, dang_mask, mid_mask = self.get_mask(state)\n             \n            h , _ = self.cbf.V_with_jacobian(state)\n\n\n            dsdt_nominal = self.nominal_dynamics(state, u, batch_size)\n            \n            dsdt_nominal = torch.reshape(dsdt_nominal,(batch_size,self.n_state))\n\n\n            alpha  = self.alpha(state)\n\n            dot_h = torch.matmul(grad_h.reshape(batch_size,1, self.n_state),dsdt_nominal.reshape(batch_size,self.n_state,1))\n            dot_h = dot_h.reshape(batch_size,1)\n\n\n            deriv_cond = dot_h + alpha * h\n\n            num_safe = torch.sum(safe_mask)\n            num_dang = torch.sum(dang_mask)\n            num_mid = torch.sum(mid_mask)\n\n            loss_alpha = torch.sum(nn.ReLU()(alpha) * safe_mask) / (1e-5 + num_safe)\n\n\n            loss_h_safe = torch.sum(nn.ReLU()(eps - h) * safe_mask) / (1e-5 + num_safe)\n            loss_h_dang = torch.sum(nn.ReLU()(h + eps) * dang_mask) / (1e-5 + num_dang)\n\n            acc_h_safe = torch.sum((h >= 0).float() * safe_mask) / (1e-5 + num_safe)\n            acc_h_dang = torch.sum((h < 0).float() * dang_mask) / (1e-5 + num_dang)\n\n            loss_deriv_safe = torch.sum(nn.ReLU()(-deriv_cond) * safe_mask) / (1e-5 + num_safe)\n            loss_deriv_dang = torch.sum(nn.ReLU()(-deriv_cond) * dang_mask) / (1e-5 + num_dang)\n            loss_deriv_mid = torch.sum(nn.ReLU()(-deriv_cond) * mid_mask) / (1e-5 + num_mid)\n\n            acc_deriv_safe = torch.sum((deriv_cond > 0).float() * safe_mask) / (1e-5 + num_safe)\n            acc_deriv_dang = torch.sum((deriv_cond > 0).float() * dang_mask) / (1e-5 + num_dang)\n            acc_deriv_mid = torch.sum((deriv_cond > 0).float() * mid_mask) / (1e-5 + num_mid)\n\n            loss = loss_alpha + loss_h_safe + loss_h_dang + loss_deriv_safe + loss_deriv_dang + loss_deriv_mid\n\n            self.cbf_optimizer.zero_grad()\n            self.alpha_optimizer.zero_grad()\n            loss.backward()\n            self.cbf_optimizer.step()\n            self.alpha_optimizer.step()\n\n            # log statics\n            acc_np[0] += acc_h_safe.detach().cpu().numpy()\n            acc_np[1] += acc_h_dang.detach().cpu().numpy()\n\n            acc_np[2] += acc_deriv_safe.detach().cpu().numpy()\n            acc_np[3] += acc_deriv_dang.detach().cpu().numpy()\n            acc_np[4] += acc_deriv_mid.detach().cpu().numpy()\n\n            loss_np += loss.detach().cpu().numpy()\n\n        acc_np = acc_np / opt_iter\n        loss_np = loss_np / opt_iter\n        return loss_np, acc_np\n\n        \n    def train_controller(self, batch_size=256, opt_iter=50, eps=0.1):\n\n        loss_np = 0.0\n        acc_np = np.zeros((5,), dtype=np.float32)\n\n        for i in range(opt_iter):\n            grad_h, state, u, u_nominal, state_next = self.dataset.sample_data(batch_size)\n            u_nominal = torch.from_numpy(u_nominal)\n            # u = torch.from_numpy(u)\n\n            if self.gpu_id >= 0:\n                grad_h = grad_h.cuda(self.gpu_id)\n                state = state.cuda(self.gpu_id)\n                u = u.cuda(self.gpu_id)\n                u_nominal = u_nominal.cuda(self.gpu_id)\n                state_next = state_next.cuda(self.gpu_id)\n\n            safe_mask, dang_mask, mid_mask = self.get_mask(state)\n\n             \n            h , _ = self.cbf.V_with_jacobian(state)\n\n\n            dsdt_nominal = self.nominal_dynamics(state, u, batch_size)\n            \n            dsdt_nominal = torch.reshape(dsdt_nominal,(batch_size,self.n_state))\n\n\n            alpha  = self.alpha(state)\n\n            dot_h = torch.matmul(grad_h.reshape(batch_size,1, self.n_state),dsdt_nominal.reshape(batch_size,self.n_state,1))\n            dot_h = dot_h.reshape(batch_size,1)\n\n\n            deriv_cond = dot_h + alpha * h\n\n\n            num_safe = torch.sum(safe_mask)\n            num_dang = torch.sum(dang_mask)\n            num_mid = torch.sum(mid_mask)\n\n            loss_deriv_safe = torch.sum(nn.ReLU()(eps_deriv - deriv_cond) * safe_mask) / (1e-5 + num_safe)\n            loss_deriv_dang = torch.sum(nn.ReLU()(eps_deriv - deriv_cond) * dang_mask) / (1e-5 + num_dang)\n            loss_deriv_mid = torch.sum(nn.ReLU()(eps_deriv - deriv_cond) * mid_mask) / (1e-5 + num_mid)\n\n            acc_deriv_safe = torch.sum((deriv_cond > 0).float() * safe_mask) / (1e-5 + num_safe)\n            acc_deriv_dang = torch.sum((deriv_cond > 0).float() * dang_mask) / (1e-5 + num_dang)\n            acc_deriv_mid = torch.sum((deriv_cond > 0).float() * mid_mask) / (1e-5 + num_mid)\n\n            loss_action = torch.mean((u - u_nominal)**2)\n\n            loss = loss_deriv_safe + loss_deriv_dang + loss_deriv_mid + loss_action * self.action_loss_weight\n\n            self.controller_optimizer.zero_grad()\n            \n            loss.backward(retain_graph=True)\n\n            loss_temp = loss.detach().cpu().numpy()\n\n            if math.isnan(loss_temp):\n                continue           \n            \n            self.controller_optimizer.step()\n            \n            # log statics\n            acc_np[0] += acc_deriv_safe.detach().cpu().numpy()\n            acc_np[1] += acc_deriv_dang.detach().cpu().numpy()\n            acc_np[2] += acc_deriv_mid.detach().cpu().numpy()\n\n            loss_np += loss.detach().cpu().numpy()\n\n        acc_np = acc_np / opt_iter\n        loss_np = loss_np / opt_iter\n\n        if self.lr_decay_stepsize >= 0:\n            # learning rate decay\n            self.cbf_lr_scheduler.step()\n            self.controller_lr_scheduler.step()\n        \n        return loss_np, acc_np\n\n    def train_cbf_and_controller(self, batch_size=1000, opt_iter=100, eps=0.1, eps_deriv=0.03, eps_action=0.2):\n        loss_np = 0.0\n        loss_h_safe_np = 0.0\n        loss_h_dang_np = 0.0\n        loss_deriv_safe_np = 0.0\n        loss_deriv_mid_np = 0.0\n        loss_deriv_dang_np = 0.0\n        loss_alpha_np = 0.0\n        loss_action_np = 0.0\n        loss_limit_np = 0.0\n        acc_np = np.zeros((5,), dtype=np.float32)\n        print(\"training\")\n        for j in range(10):\n            for i in range(opt_iter):\n                # print(i)\n                state, u, u_nominal = self.dataset.sample_data(batch_size,i)\n                u_nominal = torch.from_numpy(u_nominal)\n\n                if self.gpu_id >= 0:\n                    state = state.cuda(self.gpu_id)\n                    u = u.cuda(self.gpu_id)\n                    u_nominal = u_nominal.cuda(self.gpu_id)\n                    self.cbf.to(torch.device('cuda'))\n\n                safe_mask, dang_mask, mid_mask = self.get_mask(state)\n\n                u = self.controller(state, u_nominal.reshape(batch_size,self.m_control))\n                h , grad_h = self.cbf.V_with_jacobian(state)\n\n                dsdt = self.nominal_dynamics(state, u.reshape(batch_size,self.m_control,1), batch_size)\n                \n                dsdt = torch.reshape(dsdt,(batch_size,self.n_state))\n\n                alpha  = self.alpha(state)\n\n                dot_h = torch.matmul(grad_h.reshape(batch_size,1, self.n_state),dsdt.reshape(batch_size,self.n_state,1))\n                dot_h = dot_h.reshape(batch_size,1)\n\n\n                deriv_cond = dot_h + alpha * h\n\n\n                num_safe = torch.sum(safe_mask)\n                num_dang = torch.sum(dang_mask)\n                num_mid = torch.sum(mid_mask)\n\n                loss_h_safe = torch.sum(nn.ReLU()(eps - h).reshape(1,batch_size) * safe_mask.reshape(1,batch_size)) / (1e-5 + num_safe)\n                loss_h_dang = torch.sum(nn.ReLU()(h + eps).reshape(1,batch_size) * dang_mask.reshape(1,batch_size)) / (1e-5 + num_dang)\n\n                loss_alpha = torch.sum(nn.ReLU()(alpha).reshape(1,batch_size) * safe_mask.reshape(1,batch_size)) / (1e-5 + num_safe)\n\n                acc_h_safe = torch.sum((h >= 0).float() * safe_mask) / (1e-5 + num_safe)\n                acc_h_dang = torch.sum((h < 0).float() * dang_mask) / (1e-5 + num_dang)\n\n                loss_deriv_safe = torch.sum(nn.ReLU()(eps_deriv - deriv_cond).reshape(1,batch_size) * safe_mask.reshape(1,batch_size)) / (1e-5 + num_safe)\n                loss_deriv_dang = torch.sum(nn.ReLU()(eps_deriv - deriv_cond).reshape(1,batch_size) * dang_mask.reshape(1,batch_size)) / (1e-5 + num_dang)\n                loss_deriv_mid = torch.sum(nn.ReLU()(eps_deriv - deriv_cond).reshape(1,batch_size) * mid_mask.reshape(1,batch_size)) / (1e-5 + num_mid)\n\n                acc_deriv_safe = torch.sum((deriv_cond > 0).float() * safe_mask) / (1e-5 + num_safe)\n                acc_deriv_dang = torch.sum((deriv_cond > 0).float() * dang_mask) / (1e-5 + num_dang)\n                acc_deriv_mid = torch.sum((deriv_cond > 0).float() * mid_mask) / (1e-5 + num_mid)\n\n                # print(num_safe)\n                # print(num_dang)\n                # print(loss_alpha)\n                # print(loss_h_safe)\n                # print(loss_h_dang)\n                # print(loss_deriv_safe)\n                # print(loss_deriv_dang)\n                # print(loss_deriv_mid)\n\n                loss_action = torch.mean(nn.ReLU()(torch.abs(u - u_nominal) - eps_action))\n                loss_limit = torch.sum(nn.ReLU()(eps - u[:,0]))\n\n                loss = loss_h_safe + loss_h_dang + loss_alpha + loss_deriv_safe + loss_deriv_dang + loss_deriv_mid + loss_action * self.action_loss_weight + loss_limit\n\n                self.controller_optimizer.zero_grad()\n                self.cbf_optimizer.zero_grad()\n                self.alpha_optimizer.zero_grad()\n\n                loss.backward(retain_graph=True)\n                      \n                self.controller_optimizer.step()\n                self.cbf_optimizer.step()\n                self.alpha_optimizer.step()\n                \n                # log statics\n                acc_np[0] += acc_h_safe.detach().cpu()\n                acc_np[1] += acc_h_dang.detach().cpu()\n\n                acc_np[2] += acc_deriv_safe.detach()\n                acc_np[3] += acc_deriv_dang.detach()\n                acc_np[4] += acc_deriv_mid.detach()\n\n                loss_np += loss.detach().cpu().numpy()\n                loss_h_safe_np += loss_h_safe.detach().cpu().numpy()\n                loss_h_dang_np += loss_h_dang.detach().cpu().numpy()\n                loss_deriv_safe_np += loss_deriv_safe.detach().cpu().numpy()\n                loss_deriv_mid_np += loss_deriv_mid.detach().cpu().numpy()\n                loss_deriv_dang_np += loss_deriv_dang.detach().cpu().numpy()\n                loss_alpha_np += loss_alpha.detach().cpu().numpy()\n                loss_action_np += loss_action.detach().cpu().numpy()\n                loss_limit_np += loss_limit.detach().cpu().numpy()\n\n        acc_np /= opt_iter  * 10\n        loss_np /= opt_iter * 10\n        loss_h_safe_np /=  opt_iter * 10\n        loss_h_dang_np /= opt_iter * 10\n        loss_deriv_safe_np /= opt_iter * 10\n        loss_deriv_mid_np /= opt_iter * 10\n        loss_deriv_dang_np /= opt_iter * 10\n        loss_alpha_np /= opt_iter * 10\n        loss_action_np /= opt_iter * 10\n        loss_limit_np /= opt_iter * 10\n\n        if self.lr_decay_stepsize >= 0:\n            # learning rate decay\n            self.cbf_lr_scheduler.step()\n            self.controller_lr_scheduler.step()\n        \n        return loss_np, acc_np, loss_h_safe_np, loss_h_dang_np, loss_alpha_np, loss_deriv_safe_np, loss_deriv_dang_np, loss_deriv_mid_np, loss_action_np, loss_limit_np\n\n\n    def get_mask(self, state):\n        \"\"\"\n        args:\n            state (bs, n_state)\n        returns:\n            safe_mask (bs, k_obstacle)\n            mid_mask  (bs, k_obstacle)\n            dang_mask (bs, k_obstacle)\n        \"\"\"\n        safe_mask = self.dyn.safe_mask(state).float()\n        dang_mask = self.dyn.unsafe_mask(state).float()\n        mid_mask = (1 - safe_mask) * (1 - dang_mask)\n\n        return safe_mask, dang_mask, mid_mask\n\n    \n    def nominal_dynamics(self, state, u,batch_size):\n        \"\"\"\n        args:\n            state (n_state,)\n            u (m_control,)\n        returns:\n            dsdt (n_state,)\n        \"\"\"\n\n        m_control = self.m_control\n        fx = self.dyn._f(state,self.params)\n        gx = self.dyn._g(state, self.params)\n\n        for j in range(self.m_control):\n            if self.fault == 1 and self.fault_control_index == j:\n                u[:,j] = u[:,j].clone().detach().reshape(batch_size,1)\n            else:\n                u[:,j] = u[:,j].clone().detach().requires_grad_(True).reshape(batch_size,1)\n        \n        # if self.fault == 1 and self.fault_control_index > -1:\n            # u[:,self.fault_control_index] = u[:,self.fault_control_index].detach()\n\n        dsdt = fx + torch.matmul(gx,u)\n\n        return dsdt\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/qp_control/trainer_new.py b/qp_control/trainer_new.py
--- a/qp_control/trainer_new.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/qp_control/trainer_new.py	(date 1665758953561)
@@ -9,35 +9,36 @@
 from scipy.sparse import identity
 from scipy.sparse import vstack, csr_matrix, csc_matrix
 
-from qp_control.FxTS_GF import FxTS_Momentum 
+from qp_control.FxTS_GF import FxTS_Momentum
 
 from pytictoc import TicToc
+
 torch.autograd.set_detect_anomaly(True)
 
-t = TicToc() 
+t = TicToc()
 
 
 class Trainer(object):
 
-    def __init__(self, 
-                 controller, 
-                 cbf, 
+    def __init__(self,
+                 controller,
+                 cbf,
                  alpha,
                  dataset,
-                 dyn, 
+                 dyn,
                  n_pos,
-                 params, 
+                 params,
                  n_state,
                  m_control,
-                 j_const = 1,
-                 dt=0.05, 
-                 safe_alpha=0.3, 
-                 dang_alpha=0.4, 
+                 j_const=1,
+                 dt=0.05,
+                 safe_alpha=0.3,
+                 dang_alpha=0.4,
                  action_loss_weight=0.1,
                  gpu_id=-1,
                  lr_decay_stepsize=-1,
-                 fault = 0,
-                 fault_control_index = -1):
+                 fault=0,
+                 fault_control_index=-1):
 
         self.params = params
         self.n_state = n_state
@@ -50,19 +51,19 @@
         self.dataset = dataset
         self.fault = fault
         self.fault_control_index = fault_control_index
-        
+
         self.controller_optimizer = torch.optim.Adam(
             self.controller.parameters(), lr=5e-4, weight_decay=1e-5)
         self.cbf_optimizer = torch.optim.Adam(
             self.cbf.parameters(), lr=1e-4, weight_decay=1e-5)
         self.alpha_optimizer = torch.optim.Adam(
-            self.alpha.parameters(),lr = 1e-4, weight_decay = 1e-5)
+            self.alpha.parameters(), lr=1e-4, weight_decay=1e-5)
         # self.controller_optimizer = FxTS_Momentum(
-        #     self.controller.parameters(), lr=5e-4,momentum = 0.2)
+        #     self.controller.parameters(), lr=1e-4, momentum=0.2)
         # self.cbf_optimizer = FxTS_Momentum(
-        #     self.cbf.parameters(), lr=1e-4,momentum = 0.2)
+        #     self.cbf.parameters(), lr=1e-4, momentum=0.2)
         # self.alpha_optimizer = FxTS_Momentum(
-        #     self.alpha.parameters(), lr=5e-4,momentum = 0.2)
+        #     self.alpha.parameters(), lr=1e-4, momentum=0.2)
 
         self.n_pos = n_pos
         self.dt = dt
@@ -81,167 +82,6 @@
             self.controller_lr_scheduler = torch.optim.lr_scheduler.StepLR(
                 self.controller_optimizer, step_size=lr_decay_stepsize, gamma=0.5)
 
-
-    def train_cbf(self, batch_size=256, opt_iter=500, eps=0.1):
-
-
-        loss_np = 0.0
-        acc_np = np.zeros((5,), dtype=np.float32)
-        
-        for i in range(opt_iter):
-            grad_h, state, u, u_nominal, state_next = self.dataset.sample_data(batch_size)
-            u_nominal = torch.from_numpy(u_nominal)
-
-            if self.gpu_id >= 0:
-                grad_h = grad_h.cuda(self.gpu_id)
-                state = state.cuda(self.gpu_id)
-                u = u.cuda(self.gpu_id)
-                u_nominal = u_nominal.cuda(self.gpu_id)
-                state_next = state_next.cuda(self.gpu_id)
-
-            safe_mask, dang_mask, mid_mask = self.get_mask(state)
-             
-            h , _ = self.cbf.V_with_jacobian(state)
-
-
-            dsdt_nominal = self.nominal_dynamics(state, u, batch_size)
-            
-            dsdt_nominal = torch.reshape(dsdt_nominal,(batch_size,self.n_state))
-
-
-            alpha  = self.alpha(state)
-
-            dot_h = torch.matmul(grad_h.reshape(batch_size,1, self.n_state),dsdt_nominal.reshape(batch_size,self.n_state,1))
-            dot_h = dot_h.reshape(batch_size,1)
-
-
-            deriv_cond = dot_h + alpha * h
-
-            num_safe = torch.sum(safe_mask)
-            num_dang = torch.sum(dang_mask)
-            num_mid = torch.sum(mid_mask)
-
-            loss_alpha = torch.sum(nn.ReLU()(alpha) * safe_mask) / (1e-5 + num_safe)
-
-
-            loss_h_safe = torch.sum(nn.ReLU()(eps - h) * safe_mask) / (1e-5 + num_safe)
-            loss_h_dang = torch.sum(nn.ReLU()(h + eps) * dang_mask) / (1e-5 + num_dang)
-
-            acc_h_safe = torch.sum((h >= 0).float() * safe_mask) / (1e-5 + num_safe)
-            acc_h_dang = torch.sum((h < 0).float() * dang_mask) / (1e-5 + num_dang)
-
-            loss_deriv_safe = torch.sum(nn.ReLU()(-deriv_cond) * safe_mask) / (1e-5 + num_safe)
-            loss_deriv_dang = torch.sum(nn.ReLU()(-deriv_cond) * dang_mask) / (1e-5 + num_dang)
-            loss_deriv_mid = torch.sum(nn.ReLU()(-deriv_cond) * mid_mask) / (1e-5 + num_mid)
-
-            acc_deriv_safe = torch.sum((deriv_cond > 0).float() * safe_mask) / (1e-5 + num_safe)
-            acc_deriv_dang = torch.sum((deriv_cond > 0).float() * dang_mask) / (1e-5 + num_dang)
-            acc_deriv_mid = torch.sum((deriv_cond > 0).float() * mid_mask) / (1e-5 + num_mid)
-
-            loss = loss_alpha + loss_h_safe + loss_h_dang + loss_deriv_safe + loss_deriv_dang + loss_deriv_mid
-
-            self.cbf_optimizer.zero_grad()
-            self.alpha_optimizer.zero_grad()
-            loss.backward()
-            self.cbf_optimizer.step()
-            self.alpha_optimizer.step()
-
-            # log statics
-            acc_np[0] += acc_h_safe.detach().cpu().numpy()
-            acc_np[1] += acc_h_dang.detach().cpu().numpy()
-
-            acc_np[2] += acc_deriv_safe.detach().cpu().numpy()
-            acc_np[3] += acc_deriv_dang.detach().cpu().numpy()
-            acc_np[4] += acc_deriv_mid.detach().cpu().numpy()
-
-            loss_np += loss.detach().cpu().numpy()
-
-        acc_np = acc_np / opt_iter
-        loss_np = loss_np / opt_iter
-        return loss_np, acc_np
-
-        
-    def train_controller(self, batch_size=256, opt_iter=50, eps=0.1):
-
-        loss_np = 0.0
-        acc_np = np.zeros((5,), dtype=np.float32)
-
-        for i in range(opt_iter):
-            grad_h, state, u, u_nominal, state_next = self.dataset.sample_data(batch_size)
-            u_nominal = torch.from_numpy(u_nominal)
-            # u = torch.from_numpy(u)
-
-            if self.gpu_id >= 0:
-                grad_h = grad_h.cuda(self.gpu_id)
-                state = state.cuda(self.gpu_id)
-                u = u.cuda(self.gpu_id)
-                u_nominal = u_nominal.cuda(self.gpu_id)
-                state_next = state_next.cuda(self.gpu_id)
-
-            safe_mask, dang_mask, mid_mask = self.get_mask(state)
-
-             
-            h , _ = self.cbf.V_with_jacobian(state)
-
-
-            dsdt_nominal = self.nominal_dynamics(state, u, batch_size)
-            
-            dsdt_nominal = torch.reshape(dsdt_nominal,(batch_size,self.n_state))
-
-
-            alpha  = self.alpha(state)
-
-            dot_h = torch.matmul(grad_h.reshape(batch_size,1, self.n_state),dsdt_nominal.reshape(batch_size,self.n_state,1))
-            dot_h = dot_h.reshape(batch_size,1)
-
-
-            deriv_cond = dot_h + alpha * h
-
-
-            num_safe = torch.sum(safe_mask)
-            num_dang = torch.sum(dang_mask)
-            num_mid = torch.sum(mid_mask)
-
-            loss_deriv_safe = torch.sum(nn.ReLU()(eps_deriv - deriv_cond) * safe_mask) / (1e-5 + num_safe)
-            loss_deriv_dang = torch.sum(nn.ReLU()(eps_deriv - deriv_cond) * dang_mask) / (1e-5 + num_dang)
-            loss_deriv_mid = torch.sum(nn.ReLU()(eps_deriv - deriv_cond) * mid_mask) / (1e-5 + num_mid)
-
-            acc_deriv_safe = torch.sum((deriv_cond > 0).float() * safe_mask) / (1e-5 + num_safe)
-            acc_deriv_dang = torch.sum((deriv_cond > 0).float() * dang_mask) / (1e-5 + num_dang)
-            acc_deriv_mid = torch.sum((deriv_cond > 0).float() * mid_mask) / (1e-5 + num_mid)
-
-            loss_action = torch.mean((u - u_nominal)**2)
-
-            loss = loss_deriv_safe + loss_deriv_dang + loss_deriv_mid + loss_action * self.action_loss_weight
-
-            self.controller_optimizer.zero_grad()
-            
-            loss.backward(retain_graph=True)
-
-            loss_temp = loss.detach().cpu().numpy()
-
-            if math.isnan(loss_temp):
-                continue           
-            
-            self.controller_optimizer.step()
-            
-            # log statics
-            acc_np[0] += acc_deriv_safe.detach().cpu().numpy()
-            acc_np[1] += acc_deriv_dang.detach().cpu().numpy()
-            acc_np[2] += acc_deriv_mid.detach().cpu().numpy()
-
-            loss_np += loss.detach().cpu().numpy()
-
-        acc_np = acc_np / opt_iter
-        loss_np = loss_np / opt_iter
-
-        if self.lr_decay_stepsize >= 0:
-            # learning rate decay
-            self.cbf_lr_scheduler.step()
-            self.controller_lr_scheduler.step()
-        
-        return loss_np, acc_np
-
     def train_cbf_and_controller(self, batch_size=1000, opt_iter=100, eps=0.1, eps_deriv=0.03, eps_action=0.2):
         loss_np = 0.0
         loss_h_safe_np = 0.0
@@ -257,7 +97,7 @@
         for j in range(10):
             for i in range(opt_iter):
                 # print(i)
-                state, u, u_nominal = self.dataset.sample_data(batch_size,i)
+                state, u, u_nominal = self.dataset.sample_data(batch_size, i)
                 u_nominal = torch.from_numpy(u_nominal)
 
                 if self.gpu_id >= 0:
@@ -268,53 +108,52 @@
 
                 safe_mask, dang_mask, mid_mask = self.get_mask(state)
 
-                u = self.controller(state, u_nominal.reshape(batch_size,self.m_control))
-                h , grad_h = self.cbf.V_with_jacobian(state)
+                u = self.controller(state, u_nominal.reshape(batch_size, self.m_control))
+                h, grad_h = self.cbf.V_with_jacobian(state)
 
-                dsdt = self.nominal_dynamics(state, u.reshape(batch_size,self.m_control,1), batch_size)
-                
-                dsdt = torch.reshape(dsdt,(batch_size,self.n_state))
+                dsdt = self.nominal_dynamics(state, u.reshape(batch_size, self.m_control, 1), batch_size)
+
+                dsdt = torch.reshape(dsdt, (batch_size, self.n_state))
 
-                alpha  = self.alpha(state)
+                alpha = self.alpha(state)
 
-                dot_h = torch.matmul(grad_h.reshape(batch_size,1, self.n_state),dsdt.reshape(batch_size,self.n_state,1))
-                dot_h = dot_h.reshape(batch_size,1)
+                dot_h = torch.matmul(grad_h.reshape(batch_size, 1, self.n_state),
+                                     dsdt.reshape(batch_size, self.n_state, 1))
+                dot_h = dot_h.reshape(batch_size, 1)
 
-
                 deriv_cond = dot_h + alpha * h
 
-
                 num_safe = torch.sum(safe_mask)
                 num_dang = torch.sum(dang_mask)
                 num_mid = torch.sum(mid_mask)
 
-                loss_h_safe = torch.sum(nn.ReLU()(eps - h).reshape(1,batch_size) * safe_mask.reshape(1,batch_size)) / (1e-5 + num_safe)
-                loss_h_dang = torch.sum(nn.ReLU()(h + eps).reshape(1,batch_size) * dang_mask.reshape(1,batch_size)) / (1e-5 + num_dang)
+                loss_h_safe = torch.sum(
+                    nn.ReLU()(eps - h).reshape(1, batch_size) * safe_mask.reshape(1, batch_size)) / (1e-5 + num_safe)
+                loss_h_dang = torch.sum(
+                    nn.ReLU()(h + eps).reshape(1, batch_size) * dang_mask.reshape(1, batch_size)) / (1e-5 + num_dang)
 
-                loss_alpha = torch.sum(nn.ReLU()(alpha).reshape(1,batch_size) * safe_mask.reshape(1,batch_size)) / (1e-5 + num_safe)
+                loss_alpha = torch.sum(nn.ReLU()(alpha).reshape(1, batch_size) * safe_mask.reshape(1, batch_size)) / (
+                        1e-5 + num_safe)
 
                 acc_h_safe = torch.sum((h >= 0).float() * safe_mask) / (1e-5 + num_safe)
                 acc_h_dang = torch.sum((h < 0).float() * dang_mask) / (1e-5 + num_dang)
 
-                loss_deriv_safe = torch.sum(nn.ReLU()(eps_deriv - deriv_cond).reshape(1,batch_size) * safe_mask.reshape(1,batch_size)) / (1e-5 + num_safe)
-                loss_deriv_dang = torch.sum(nn.ReLU()(eps_deriv - deriv_cond).reshape(1,batch_size) * dang_mask.reshape(1,batch_size)) / (1e-5 + num_dang)
-                loss_deriv_mid = torch.sum(nn.ReLU()(eps_deriv - deriv_cond).reshape(1,batch_size) * mid_mask.reshape(1,batch_size)) / (1e-5 + num_mid)
+                loss_deriv_safe = torch.sum(
+                    nn.ReLU()(eps_deriv - deriv_cond).reshape(1, batch_size) * safe_mask.reshape(1, batch_size)) / (
+                                          1e-5 + num_safe)
+                loss_deriv_dang = torch.sum(
+                    nn.ReLU()(eps_deriv - deriv_cond).reshape(1, batch_size) * dang_mask.reshape(1, batch_size)) / (
+                                          1e-5 + num_dang)
+                loss_deriv_mid = torch.sum(
+                    nn.ReLU()(eps_deriv - deriv_cond).reshape(1, batch_size) * mid_mask.reshape(1, batch_size)) / (
+                                         1e-5 + num_mid)
 
                 acc_deriv_safe = torch.sum((deriv_cond > 0).float() * safe_mask) / (1e-5 + num_safe)
                 acc_deriv_dang = torch.sum((deriv_cond > 0).float() * dang_mask) / (1e-5 + num_dang)
                 acc_deriv_mid = torch.sum((deriv_cond > 0).float() * mid_mask) / (1e-5 + num_mid)
 
-                # print(num_safe)
-                # print(num_dang)
-                # print(loss_alpha)
-                # print(loss_h_safe)
-                # print(loss_h_dang)
-                # print(loss_deriv_safe)
-                # print(loss_deriv_dang)
-                # print(loss_deriv_mid)
-
                 loss_action = torch.mean(nn.ReLU()(torch.abs(u - u_nominal) - eps_action))
-                loss_limit = torch.sum(nn.ReLU()(eps - u[:,0]))
+                loss_limit = torch.sum(nn.ReLU()(eps - u[:, 0]))
 
                 loss = loss_h_safe + loss_h_dang + loss_alpha + loss_deriv_safe + loss_deriv_dang + loss_deriv_mid + loss_action * self.action_loss_weight + loss_limit
 
@@ -323,11 +162,11 @@
                 self.alpha_optimizer.zero_grad()
 
                 loss.backward(retain_graph=True)
-                      
+
                 self.controller_optimizer.step()
                 self.cbf_optimizer.step()
                 self.alpha_optimizer.step()
-                
+
                 # log statics
                 acc_np[0] += acc_h_safe.detach().cpu()
                 acc_np[1] += acc_h_dang.detach().cpu()
@@ -346,9 +185,9 @@
                 loss_action_np += loss_action.detach().cpu().numpy()
                 loss_limit_np += loss_limit.detach().cpu().numpy()
 
-        acc_np /= opt_iter  * 10
+        acc_np /= opt_iter * 10
         loss_np /= opt_iter * 10
-        loss_h_safe_np /=  opt_iter * 10
+        loss_h_safe_np /= opt_iter * 10
         loss_h_dang_np /= opt_iter * 10
         loss_deriv_safe_np /= opt_iter * 10
         loss_deriv_mid_np /= opt_iter * 10
@@ -361,10 +200,9 @@
             # learning rate decay
             self.cbf_lr_scheduler.step()
             self.controller_lr_scheduler.step()
-        
+
         return loss_np, acc_np, loss_h_safe_np, loss_h_dang_np, loss_alpha_np, loss_deriv_safe_np, loss_deriv_dang_np, loss_deriv_mid_np, loss_action_np, loss_limit_np
 
-
     def get_mask(self, state):
         """
         args:
@@ -380,8 +218,7 @@
 
         return safe_mask, dang_mask, mid_mask
 
-    
-    def nominal_dynamics(self, state, u,batch_size):
+    def nominal_dynamics(self, state, u, batch_size):
         """
         args:
             state (n_state,)
@@ -391,18 +228,18 @@
         """
 
         m_control = self.m_control
-        fx = self.dyn._f(state,self.params)
+        fx = self.dyn._f(state, self.params)
         gx = self.dyn._g(state, self.params)
 
         for j in range(self.m_control):
             if self.fault == 1 and self.fault_control_index == j:
-                u[:,j] = u[:,j].clone().detach().reshape(batch_size,1)
+                u[:, j] = u[:, j].clone().detach().reshape(batch_size, 1)
             else:
-                u[:,j] = u[:,j].clone().detach().requires_grad_(True).reshape(batch_size,1)
-        
+                u[:, j] = u[:, j].clone().detach().requires_grad_(True).reshape(batch_size, 1)
+
         # if self.fault == 1 and self.fault_control_index > -1:
-            # u[:,self.fault_control_index] = u[:,self.fault_control_index].detach()
+        # u[:,self.fault_control_index] = u[:,self.fault_control_index].detach()
 
-        dsdt = fx + torch.matmul(gx,u)
+        dsdt = fx + torch.matmul(gx, u)
 
         return dsdt
Index: qp_control/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport math\nimport scipy\nimport numpy as np\nfrom qpsolvers import solve_qp\nfrom osqp import OSQP\nfrom scipy.sparse import identity\nfrom scipy.sparse import vstack, csr_matrix, csc_matrix\n\n\nclass Utils(object):\n\n    def __init__(self, \n                 dyn, \n                 params, \n                 n_state,\n                 m_control,\n                 j_const = 1,\n                 dt=0.05, \n                 fault = 0,\n                 fault_control_index = -1):\n\n        self.params = params\n        self.n_state = n_state\n        self.m_control = m_control\n        self.j_const = j_const\n        self.dyn = dyn\n        self.fault = fault\n        self.fault_control_index = fault_control_index\n        self.dt = dt\n\n\n    def is_safe(self, state):\n\n        # alpha = torch.abs(state[:,1])\n        return self.dyn.safe_mask(state)\n\n    def is_unsafe(self, state):\n\n        # alpha = torch.abs(state[:,1])\n        return self.dyn.unsafe_mask(state)\n\n    \n    def nominal_dynamics(self, state, u,batch_size):\n        \"\"\"\n        args:\n            state (n_state,)\n            u (m_control,)\n        returns:\n            dsdt (n_state,)\n        \"\"\"\n\n        m_control = self.m_control\n        fx = self.dyn._f(state,self.params)\n        gx = self.dyn._g(state, self.params)\n\n        for j in range(self.m_control):\n            if self.fault == 1 and self.fault_control_index == j:\n                u[:,j] = u[:,j].clone().detach().reshape(batch_size,1)\n            else:\n                u[:,j] = u[:,j].clone().detach().requires_grad_(True).reshape(batch_size,1)\n        \n        dsdt = fx + torch.matmul(gx,u)\n\n        return dsdt\n\n    \n    def nominal_controller(self, state, goal, u_n, dyn,constraints):\n        \"\"\"\n        args:\n            state (n_state,)\n            goal (n_state,)\n        returns:\n            u_nominal (m_control,)\n        \"\"\"\n        um, ul = self.dyn.control_limits()\n        sm, sl = self.dyn.state_limits()\n\n        n_state = self.n_state\n        m_control = self.m_control\n        params = self.params\n        j_const = self.j_const\n\n        size_Q = m_control + j_const\n\n        Q = csc_matrix(identity(size_Q))\n        Q[0,0] = 1 / um[0]\n        # print(Q)\n        # print(asas)\n\n        F = torch.ones(size_Q,1)\n        \n        F[0:m_control] = - u_n.reshape(m_control,1)\n        F = np.array(F)\n        F[0] = F[0] / um[0]\n\n        F[-1] = - 10\n\n        fx = dyn._f(state,params)\n        gx = dyn._g(state,params)\n\n        fx = fx.reshape(n_state,1)\n        gx = gx.reshape(n_state,m_control)\n\n        V, Lg, Lf = constraints.LfLg_new(state,goal,fx,gx,n_state, m_control, j_const, 1, [np.pi / 8, -np.pi / 80])\n\n        # if V == 0:\n        #     V[] = 1e-4\n\n        A = torch.hstack((- Lg, - V))\n        B = Lf\n\n        G = scipy.sparse.csc.csc_matrix(A)\n        h = - np.array(B)\n        u = solve_qp(Q, F, G, h, solver=\"osqp\")\n\n        if (u is None):\n            u = u_n.reshape(1,m_control)\n        #     u = u.reshape(1,m_control)\n        u = u[0:m_control]\n        # print(u)\n        u_nominal = torch.tensor(u).reshape(1,m_control)\n\n        return u_nominal\n\n\n    def neural_controller(self, u_nominal, fx, gx, h, grad_h, fault_start):\n        \"\"\"\n        args:\n            state (n_state,)\n            goal (n_state,)\n        returns:\n            u_nominal (m_control,)\n        \"\"\"\n        um, ul = self.dyn.control_limits()\n        n_state = self.n_state\n        m_control = self.m_control\n        params = self.params\n        j_const = self.j_const\n\n        size_Q = m_control + 1\n\n        Q = csc_matrix(identity(size_Q))\n        # Q[0,0] = 1 / um[0]\n        F = torch.hstack((torch.tensor(u_nominal).reshape(m_control), torch.tensor(1.0))).reshape(size_Q,1)\n\n        F = - np.array(F)\n        # F[0] = F[0] / um[0]\n        \n\n        Q = Q / 100 \n        F = F / 100\n\n        F[-1] = -1\n\n        Lg = torch.matmul(grad_h, gx)\n        Lf = torch.matmul(grad_h, fx)\n\n        if fault_start == 1:\n            # Lg[]\n            # print(Lg.shape)\n            Lf = Lf - torch.abs(Lg[0,0,self.fault_control_index]) * um[self.fault_control_index]\n            Lg[0,0,self.fault_control_index] = 0\n\n\n        if h == 0:\n            h = 1e-4\n\n        A = torch.hstack((- Lg.reshape(1,m_control), -h))\n        A = torch.tensor(A.detach().cpu())\n        B = Lf.detach().cpu().numpy() \n        B = np.array(B)\n\n        # print(A)\n        A = scipy.sparse.csc.csc_matrix(A)\n        u = solve_qp(Q, F, A, B, solver=\"osqp\")\n\n        # print(A)\n        # print(B)\n        # print(u)\n        # print(asasa)\n\n        if (u is None):\n            u_neural = u_nominal.reshape(m_control)\n        else:\n            u_neural = torch.tensor([u[0:self.m_control]]).reshape(1,m_control)\n            # u = np.array(um.clone()) / 2\n        #     u = u.reshape(1,m_control)\n        # print(u.shape)\n\n        \n\n        return u_neural
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/qp_control/utils.py b/qp_control/utils.py
--- a/qp_control/utils.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/qp_control/utils.py	(date 1666107958274)
@@ -1,4 +1,7 @@
+import pdb
+
 import torch
+import torch.distributions as td
 import math
 import scipy
 import numpy as np
@@ -10,15 +13,15 @@
 
 class Utils(object):
 
-    def __init__(self, 
-                 dyn, 
-                 params, 
+    def __init__(self,
+                 dyn,
+                 params,
                  n_state,
                  m_control,
-                 j_const = 1,
-                 dt=0.05, 
-                 fault = 0,
-                 fault_control_index = -1):
+                 j_const=1,
+                 dt=0.05,
+                 fault=0,
+                 fault_control_index=-1):
 
         self.params = params
         self.n_state = n_state
@@ -29,7 +32,6 @@
         self.fault_control_index = fault_control_index
         self.dt = dt
 
-
     def is_safe(self, state):
 
         # alpha = torch.abs(state[:,1])
@@ -40,8 +42,7 @@
         # alpha = torch.abs(state[:,1])
         return self.dyn.unsafe_mask(state)
 
-    
-    def nominal_dynamics(self, state, u,batch_size):
+    def nominal_dynamics(self, state, u, batch_size):
         """
         args:
             state (n_state,)
@@ -51,21 +52,20 @@
         """
 
         m_control = self.m_control
-        fx = self.dyn._f(state,self.params)
+        fx = self.dyn._f(state, self.params)
         gx = self.dyn._g(state, self.params)
 
         for j in range(self.m_control):
             if self.fault == 1 and self.fault_control_index == j:
-                u[:,j] = u[:,j].clone().detach().reshape(batch_size,1)
+                u[:, j] = u[:, j].clone().detach().reshape(batch_size, 1)
             else:
-                u[:,j] = u[:,j].clone().detach().requires_grad_(True).reshape(batch_size,1)
-        
-        dsdt = fx + torch.matmul(gx,u)
+                u[:, j] = u[:, j].clone().detach().requires_grad_(True).reshape(batch_size, 1)
+
+        dsdt = fx + torch.matmul(gx, u)
 
         return dsdt
 
-    
-    def nominal_controller(self, state, goal, u_n, dyn,constraints):
+    def nominal_controller(self, state, goal, u_n, dyn, constraints):
         """
         args:
             state (n_state,)
@@ -81,49 +81,49 @@
         params = self.params
         j_const = self.j_const
 
+        batch_size = state.shape[0]
+
         size_Q = m_control + j_const
 
         Q = csc_matrix(identity(size_Q))
-        Q[0,0] = 1 / um[0]
-        # print(Q)
-        # print(asas)
+        Q[0, 0] = 1 / um[0]
 
-        F = torch.ones(size_Q,1)
-        
-        F[0:m_control] = - u_n.reshape(m_control,1)
-        F = np.array(F)
-        F[0] = F[0] / um[0]
-
-        F[-1] = - 10
-
-        fx = dyn._f(state,params)
-        gx = dyn._g(state,params)
+        F = torch.ones(size_Q, 1)
+        u_nominal = u_n
+        for i in range(batch_size):
+            state_i = state[i, :].reshape(1,n_state)
+            F[0:m_control] = - u_n[i, :].reshape(m_control, 1)
+            F = np.array(F)
+            F[0] = F[0] / um[0]
+            F[-1] = - 10
+            fx = dyn._f(state_i, params)
+            gx = dyn._g(state_i, params)
 
-        fx = fx.reshape(n_state,1)
-        gx = gx.reshape(n_state,m_control)
+            fx = fx.reshape(n_state, 1)
+            gx = gx.reshape(n_state, m_control)
 
-        V, Lg, Lf = constraints.LfLg_new(state,goal,fx,gx,n_state, m_control, j_const, 1, [np.pi / 8, -np.pi / 80])
+            V, Lg, Lf = constraints.LfLg_new(state_i, goal, fx, gx, n_state, m_control, j_const, 1, [np.pi / 8, -np.pi / 80])
 
-        # if V == 0:
-        #     V[] = 1e-4
+            # if V == 0:
+            #     V[] = 1e-4
 
-        A = torch.hstack((- Lg, - V))
-        B = Lf
+            A = torch.hstack((- Lg, - V))
+            B = Lf
 
-        G = scipy.sparse.csc.csc_matrix(A)
-        h = - np.array(B)
-        u = solve_qp(Q, F, G, h, solver="osqp")
+            G = scipy.sparse.csc.csc_matrix(A)
+            h = - np.array(B)
+            # u = scipy.optimize.linprog(F, A_ub=G, b_ub=h)
+            u = solve_qp(Q, F, G, h, solver="osqp")
 
-        if (u is None):
-            u = u_n.reshape(1,m_control)
-        #     u = u.reshape(1,m_control)
-        u = u[0:m_control]
-        # print(u)
-        u_nominal = torch.tensor(u).reshape(1,m_control)
+            if u is None:
+                u = u_n[i, :].reshape(1, m_control)
+            #     u = u.reshape(1,m_control)
+            u = u[0:m_control]
+            # print(u)
+            u_nominal[i, :] = torch.tensor(u).reshape(1, m_control)
 
         return u_nominal
 
-
     def neural_controller(self, u_nominal, fx, gx, h, grad_h, fault_start):
         """
         args:
@@ -142,13 +142,12 @@
 
         Q = csc_matrix(identity(size_Q))
         # Q[0,0] = 1 / um[0]
-        F = torch.hstack((torch.tensor(u_nominal).reshape(m_control), torch.tensor(1.0))).reshape(size_Q,1)
+        F = torch.hstack((torch.tensor(u_nominal).reshape(m_control), torch.tensor(1.0))).reshape(size_Q, 1)
 
         F = - np.array(F)
         # F[0] = F[0] / um[0]
-        
 
-        Q = Q / 100 
+        Q = Q / 100
         F = F / 100
 
         F[-1] = -1
@@ -159,16 +158,15 @@
         if fault_start == 1:
             # Lg[]
             # print(Lg.shape)
-            Lf = Lf - torch.abs(Lg[0,0,self.fault_control_index]) * um[self.fault_control_index]
-            Lg[0,0,self.fault_control_index] = 0
+            Lf = Lf - torch.abs(Lg[0, 0, self.fault_control_index]) * um[self.fault_control_index]
+            Lg[0, 0, self.fault_control_index] = 0
 
-
         if h == 0:
             h = 1e-4
 
-        A = torch.hstack((- Lg.reshape(1,m_control), -h))
+        A = torch.hstack((- Lg.reshape(1, m_control), -h))
         A = torch.tensor(A.detach().cpu())
-        B = Lf.detach().cpu().numpy() 
+        B = Lf.detach().cpu().numpy()
         B = np.array(B)
 
         # print(A)
@@ -180,14 +178,70 @@
         # print(u)
         # print(asasa)
 
-        if (u is None):
+        if u is None:
             u_neural = u_nominal.reshape(m_control)
         else:
-            u_neural = torch.tensor([u[0:self.m_control]]).reshape(1,m_control)
+            u_neural = torch.tensor([u[0:self.m_control]]).reshape(1, m_control)
             # u = np.array(um.clone()) / 2
         #     u = u.reshape(1,m_control)
         # print(u.shape)
 
-        
+        return u_neural
+
+    def x_bndr(self, sm, sl, N):
+        """
+        args:
+            state lower limit sl
+            state upper limit sm
+        returns:
+            samples on boundary x
+        """
 
-        return u_neural
\ No newline at end of file
+        n_dims = self.n_state
+        batch = N
+
+        normal_idx = torch.randint(0, n_dims, size=(batch,))
+        assert normal_idx.shape == (batch,)
+
+        # 2: Choose whether it takes the value of hi or lo.
+        direction = torch.randint(2, size=(batch,), dtype=torch.bool)
+        assert direction.shape == (batch,)
+
+        lo = sl
+        hi = sm
+        assert lo.shape == hi.shape == (n_dims,)
+        dist = td.Uniform(lo, hi)
+
+        samples = dist.sample((batch,))
+        assert samples.shape == (batch, n_dims)
+
+        tmp = torch.where(direction, hi[normal_idx], lo[normal_idx])
+        assert tmp.shape == (batch,)
+
+        # print(tmp.shape)
+        # tmp = 13 * torch.ones(batch)
+        tmp = tmp[:, None].repeat(1, n_dims)
+
+        # print("samples")
+        # print(samples)
+        # print("samples2")
+        # print(samples[:, normal_idx])
+        # print(samples[:, normal_idx].shape)
+
+        # tmp2 = torch.arange(batch * n_dims).reshape((batch, n_dims)).float()
+
+        # print(normal_idx)
+        # print(samples.shape)
+
+        # samples[:, normal_idx] = tmp
+        samples.scatter_(1, normal_idx[:, None], tmp)
+
+        # eq_lo = samples == sl
+        # eq_hi = samples == sm
+        #
+        # n_on_bdry = torch.sum(eq_lo, dim=1) + torch.sum(eq_hi, dim=1)
+        # all_on_bdry = torch.all(n_on_bdry >= 1)
+        # print("all_on_bdry: ", all_on_bdry)
+
+        return samples
+
Index: Fixed_wing_test_plot.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\nimport torch\nimport math\nimport random\nimport numpy as np\nfrom dynamics.fixed_wing import FixedWing\nfrom qp_control import config\nfrom qp_control.constraints_fw import constraints\nfrom qp_control.datagen import Dataset_with_Grad\nfrom qp_control.trainer_new import Trainer\nfrom qp_control.utils import Utils\nfrom qp_control.NNfuncgrad import CBF, alpha_param, NNController_new\nimport matplotlib.pyplot as plt\nsys.path.insert(1, os.path.abspath('.'))\n\n\nxg = torch.tensor([[100.0,\n                    0.2,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0]])\n\nx0 = torch.tensor([[50.0,\n                    0.2,\n                    0.1,\n                    0.4,\n                    0.5,\n                    0.2,\n                    0.1,\n                    0.5,\n                    0.9]])\n\ndt = 0.01\nn_state = 9\nm_control = 4\nfault = 1\n\nnominal_params = {\n    \"m\": 100.0,\n    \"g\": 9.8,\n    \"Ixx\": 100,\n    \"Iyy\": 1000,\n    \"Izz\": 1000,\n    \"Ixz\": 100,\n    \"S\": 100,\n    \"b\": 5,\n    \"bar_c\": 5,\n    \"rho\": 1.2,\n    \"Cd0\": 0.0434,\n    \"Cda\": 0.22,\n    \"Clb\": -0.13,\n    \"Clp\": -0.505,\n    \"Clr\": 0.252,\n    \"Clda\": 0.0855,\n    \"Cldr\": -0.0024,\n    \"Cm0\": 0.135,\n    \"Cma\": -1.50,\n    \"Cmq\": -38.2,\n    \"Cmde\": -0.992,\n    \"Cnb\": 0.0726,\n    \"Cnp\": -0.069,\n    \"Cnr\": -0.0946,\n    \"Cnda\": 0.7,\n    \"Cndr\": -0.0693,\n    \"Cyb\": -0.83,\n    \"Cyp\": 0,\n    \"Cyr\": 0,\n    \"Cydr\": 0.1,\n    \"Cz0\": 0.23,\n    \"Cza\": 4.58,\n    \"Czq\": 0,\n    \"Czde\": 0.1,\n    \"Cx0\": 0,\n    \"Cxq\": 0,\n    \"fault\": fault, }\n\nfault_control_index = 1\nfault_duration = config.FAULT_DURATION\n\nfault_known = 1\n\n\ndef main():\n    dynamics = FixedWing(x=x0, nominal_params=nominal_params, dt=dt, controller_dt=dt)\n    util = Utils(n_state=n_state, m_control=m_control, dyn=dynamics, params=nominal_params, fault=fault,\n                 fault_control_index=fault_control_index)\n\n    NN_controller = NNController_new(n_state=n_state, m_control=m_control)\n    NN_cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault=0, fault_control_index=fault_control_index)\n    # NN_alpha = alpha_param(n_state=n_state)\n\n    NN_controller.load_state_dict(torch.load('./data/FW_controller_NN_weights.pth'))\n    NN_cbf.load_state_dict(torch.load('./data/FW_cbf_NN_weights.pth'))\n    # NN_alpha.load_state_dict(torch.load('./data/data/CF_alpha_NN_weights.pth'))\n\n    NN_cbf.eval()\n    NN_controller.eval()\n    # NN_alpha.eval()\n\n    FT_controller = NNController_new(n_state=n_state, m_control=m_control)\n    FT_cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault=1, fault_control_index=fault_control_index)\n    # FT_alpha = alpha_param(n_state=n_state)\n\n    FT_controller.load_state_dict(torch.load('./data/FW_controller_FT_weights.pth'))\n    FT_cbf.load_state_dict(torch.load('./data/FW_cbf_FT_weights.pth'))\n    # FT_alpha.load_state_dict(torch.load('./data/data/CF_alpha_FT_weights.pth'))\n\n    FT_cbf.eval()\n    FT_controller.eval()\n    # FT_alpha.eval()\n\n    state = x0\n    goal = xg\n    goal = np.array(goal).reshape(1, n_state)\n\n    safety_rate = 0.0\n    unsafety_rate = 0.0\n    h_correct = 0.0\n    goal_reached = 0\n    num_episodes = 0\n    traj_following_error = 0\n    epsilon = 0.1\n\n    um, ul = dynamics.control_limits()\n\n    sm, sl = dynamics.state_limits()\n\n    x_pl = np.array(state).reshape(1, n_state)\n    fault_activity = np.array([0])\n    u_pl = np.array([0] * m_control).reshape(1, m_control)\n    h, _ = NN_cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n\n    # print(h)\n    h_pl = np.array(h.detach()).reshape(1, 1)\n\n    rand_start = random.uniform(1.01, 100)\n\n    fault_start_epoch = math.floor(config.EVAL_STEPS / rand_start)# + 100000000\n    fault_start = 0\n    u_nominal = torch.zeros(1, m_control)\n\n    for i in range(config.EVAL_STEPS):\n        # print(i)\n\n        for j in range(n_state):\n            if state[0, j] < sl[j]:\n                state[0, j] = sl[j].clone()\n            if state[0, j] > sm[j]:\n                state[0, j] = sm[j].clone()\n\n        fx = dynamics._f(state, params=nominal_params)\n        gx = dynamics._g(state, params=nominal_params)\n\n        u_nominal = util.nominal_controller(state=state, goal=goal, u_n=u_nominal, dyn=dynamics, constraints=constraints)\n        # if fault_start == 0:\n        # u_nominal = NN_controller(torch.tensor(state, dtype=torch.float32),\n        #                           torch.tensor(u_nominal, dtype=torch.float32))\n        # else:\n        # u_nominal = FT_controller(torch.tensor(state, dtype=torch.float32), torch.tensor(u_nominal, dtype=torch.float32))\n\n        for j in range(m_control):\n            if u_nominal[0, j] < ul[j]:\n                u_nominal[0, j] = ul[j].clone()\n            if u_nominal[0, j] > um[j]:\n                u_nominal[0, j] = um[j].clone()\n\n        if fault_known == 1:\n            ## 1 -> time-based switching, assumes knowledge of when fault occurs and stops\n            ## 0 -> Fault-detection based-switching, using the proposed scheme from the paper\n\n            if fault_start == 0 and fault_start_epoch <= i <= (\n                    fault_start_epoch + fault_duration):  # and util.is_safe(state):\n                fault_start = 1\n\n            # print(i)\n            # print(fault_start_epoch + fault_duration)\n            # print(fault_start)\n\n            if fault_start == 1 and i > (fault_start_epoch + fault_duration):\n                # print(\"here\")\n                fault_start = 0\n\n            if fault_start == 0:\n                h, grad_h = NN_cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n            else:\n                h, grad_h = FT_cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n\n            u = util.neural_controller(u_nominal, fx, gx, h, grad_h, fault_start)\n            u = torch.squeeze(u.detach().cpu())\n\n            if fault_start == 1:\n                u[fault_control_index] = torch.rand(1) * 5\n\n            for j in range(m_control):\n                if u[j] < ul[j]:\n                    u[j] = ul[j].clone()\n                if u[j] > um[j]:\n                    u[j] = um[j].clone()\n\n            # if torch.isnan(torch.sum(u)):\n            # \ti = i-1\n            # \tcontinue\n\n            u = torch.tensor(u, dtype=torch.float32)\n            gxu = torch.matmul(gx, u.reshape(m_control, 1))\n\n            dx = fx.reshape(1, n_state) + gxu.reshape(1, n_state)\n\n        else:\n            h, grad_h = NN_cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n            u = util.neural_controller(u_nominal, fx, gx, h, grad_h, fault_start=fault_start)\n            u = torch.squeeze(u.detach().cpu())\n\n            if fault_start_epoch <= i <= fault_start_epoch + fault_duration:\n                u[fault_control_index] = torch.rand(1)\n\n            for j in range(m_control):\n                if u[j] <= ul[j]:\n                    u[j] = ul[j].clone()\n                if u[j] >= um[j]:\n                    u[j] = um[j].clone()\n\n            if torch.isnan(torch.sum(u)):\n                i = i - 1\n                continue\n\n            u = torch.tensor(u, dtype=torch.float32)\n            gxu = torch.matmul(gx, u.reshape(m_control, 1))\n\n            dx = fx.reshape(1, n_state) + gxu.reshape(1, n_state)\n\n            dot_h = torch.matmul(dx, grad_h.reshape(n_state, 1))\n            if dot_h < epsilon:\n                h, grad_h = FT_cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n                u = util.neural_controller(u_nominal, fx, gx, h, grad_h, fault_start=fault_start)\n                u = torch.squeeze(u.detach().cpu())\n                if fault_start_epoch <= i <= fault_start_epoch + fault_duration:\n                    u[fault_control_index] = torch.rand(1) * 5\n\n                for j in range(m_control):\n                    if u[j] < ul[j]:\n                        u[j] = ul[j].clone()\n                    if u[j] > um[j]:\n                        u[j] = um[j].clone()\n\n                if torch.isnan(torch.sum(u)):\n                    i = i - 1\n                    continue\n\n                u = torch.tensor(u, dtype=torch.float32)\n                gxu = torch.matmul(gx, u.reshape(m_control, 1))\n\n                dx = fx.reshape(1, n_state) + gxu.reshape(1, n_state)\n\n        state_next = state + dx * dt\n\n        is_safe = int(util.is_safe(state))\n        is_unsafe = int(util.is_unsafe(state))\n        safety_rate += is_safe / config.EVAL_STEPS\n        unsafety_rate += is_unsafe / config.EVAL_STEPS\n        h_correct += is_safe * int(h >= 0) / config.EVAL_STEPS + is_unsafe * int(h < 0) / config.EVAL_STEPS\n\n        x_pl = np.vstack((x_pl, np.array(state.clone().detach()).reshape(1, n_state)))\n        fault_activity = np.vstack((fault_activity, fault_start))\n        u_pl = np.vstack((u_pl, np.array(u.clone().detach()).reshape(1, m_control)))\n        h_pl = np.vstack((h_pl, np.array(h.clone().detach()).reshape(1, 1)))\n\n        state = state_next.clone()\n\n    time_pl = np.arange(0., dt * config.EVAL_STEPS + dt, dt)\n\n    alpha_pl = x_pl[:, 1]\n    print(safety_rate)\n    print(unsafety_rate)\n    print(h_correct)\n\n    u1 = u_pl[:, 0]\n    u2 = u_pl[:, 1]\n    u3 = u_pl[:, 2]\n    u4 = u_pl[:, 3]\n\n    plt.figure(figsize=(18, 6))\n\n    plt.subplot(331)\n    plt.plot(time_pl, alpha_pl)\n    plt.subplot(334)\n    plt.plot(time_pl, h_pl)\n    plt.subplot(333)\n    plt.plot(time_pl, u1, '--r')\n    plt.subplot(332)\n    plt.plot(time_pl, u2, '--g')\n    plt.subplot(335)\n    plt.plot(time_pl, u3, '--b')\n    plt.subplot(336)\n    plt.plot(time_pl, u4, '--y')\n    # plt.suptitle('Categorical Plotting')\n    plt.subplot(337)\n    plt.plot(time_pl, x_pl[:, 0], '--r')\n    plt.subplot(338)\n    plt.plot(time_pl, x_pl[:, 7], '--b')\n    plt.subplot(339)\n    plt.plot(time_pl, fault_activity, '--g')\n    plt.savefig('./plots/plot_closed_loop_data_FW.png')\n\n\nif __name__ == '__main__':\n    main()\n\n# scp -r kgarg@18.18.47.27:/home/kgarg/kunal_files/MIT_REALM/fault_tol_control/data/data /home/kunal/MIT_REALM/Research/fault_tol_control/data/\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Fixed_wing_test_plot.py b/Fixed_wing_test_plot.py
--- a/Fixed_wing_test_plot.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/Fixed_wing_test_plot.py	(date 1665756394762)
@@ -12,9 +12,9 @@
 from qp_control.utils import Utils
 from qp_control.NNfuncgrad import CBF, alpha_param, NNController_new
 import matplotlib.pyplot as plt
+
 sys.path.insert(1, os.path.abspath('.'))
 
-
 xg = torch.tensor([[100.0,
                     0.2,
                     0.0,
@@ -121,9 +121,6 @@
     safety_rate = 0.0
     unsafety_rate = 0.0
     h_correct = 0.0
-    goal_reached = 0
-    num_episodes = 0
-    traj_following_error = 0
     epsilon = 0.1
 
     um, ul = dynamics.control_limits()
@@ -140,7 +137,7 @@
 
     rand_start = random.uniform(1.01, 100)
 
-    fault_start_epoch = math.floor(config.EVAL_STEPS / rand_start)# + 100000000
+    fault_start_epoch = math.floor(config.EVAL_STEPS / rand_start)  # + 100000000
     fault_start = 0
     u_nominal = torch.zeros(1, m_control)
 
@@ -156,12 +153,11 @@
         fx = dynamics._f(state, params=nominal_params)
         gx = dynamics._g(state, params=nominal_params)
 
-        u_nominal = util.nominal_controller(state=state, goal=goal, u_n=u_nominal, dyn=dynamics, constraints=constraints)
-        # if fault_start == 0:
-        # u_nominal = NN_controller(torch.tensor(state, dtype=torch.float32),
-        #                           torch.tensor(u_nominal, dtype=torch.float32))
-        # else:
-        # u_nominal = FT_controller(torch.tensor(state, dtype=torch.float32), torch.tensor(u_nominal, dtype=torch.float32))
+        u_nominal = util.nominal_controller(state=state, goal=goal, u_n=u_nominal, dyn=dynamics,
+                                            constraints=constraints)
+        # if fault_start == 0: u_nominal = NN_controller(torch.tensor(state, dtype=torch.float32), torch.tensor(
+        # u_nominal, dtype=torch.float32)) else: u_nominal = FT_controller(torch.tensor(state, dtype=torch.float32),
+        # torch.tensor(u_nominal, dtype=torch.float32))
 
         for j in range(m_control):
             if u_nominal[0, j] < ul[j]:
@@ -170,8 +166,8 @@
                 u_nominal[0, j] = um[j].clone()
 
         if fault_known == 1:
-            ## 1 -> time-based switching, assumes knowledge of when fault occurs and stops
-            ## 0 -> Fault-detection based-switching, using the proposed scheme from the paper
+            # 1 -> time-based switching, assumes knowledge of when fault occurs and stops
+            # 0 -> Fault-detection based-switching, using the proposed scheme from the paper
 
             if fault_start == 0 and fault_start_epoch <= i <= (
                     fault_start_epoch + fault_duration):  # and util.is_safe(state):
Index: qp_control/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>TRAIN_STEPS = 1000000\n#TRAIN_STEPS = 500000\nEVAL_STEPS = 1000\nEVAL_EPOCHS = 100\nPOLICY_UPDATE_INTERVAL = 20000\nINIT_STATE_UPDATE = 500\nFAULT_DURATION = EVAL_STEPS / 10\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/qp_control/config.py b/qp_control/config.py
--- a/qp_control/config.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/qp_control/config.py	(date 1666034870637)
@@ -5,3 +5,44 @@
 POLICY_UPDATE_INTERVAL = 20000
 INIT_STATE_UPDATE = 500
 FAULT_DURATION = EVAL_STEPS / 10
+
+fault = int(input("Fault (1) or pre-fault (0):"))
+
+FIXED_WING_PARAMS = {
+    "m": 100.0,
+    "g": 9.8,
+    "Ixx": 100,
+    "Iyy": 1000,
+    "Izz": 1000,
+    "Ixz": 100,
+    "S": 100,
+    "b": 5,
+    "bar_c": 5,
+    "rho": 1.2,
+    "Cd0": 0.0434,
+    "Cda": 0.22,
+    "Clb": -0.13,
+    "Clp": -0.505,
+    "Clr": 0.252,
+    "Clda": 0.0855,
+    "Cldr": -0.0024,
+    "Cm0": 0.135,
+    "Cma": -1.50,
+    "Cmq": -38.2,
+    "Cmde": -0.992,
+    "Cnb": 0.0726,
+    "Cnp": -0.069,
+    "Cnr": -0.0946,
+    "Cnda": 0.7,
+    "Cndr": -0.0693,
+    "Cyb": -0.83,
+    "Cyp": 0,
+    "Cyr": 0,
+    "Cydr": 0.1,
+    "Cz0": 0.23,
+    "Cza": 4.58,
+    "Czq": 0,
+    "Czde": 0.1,
+    "Cx0": 0,
+    "Cxq": 0,
+    "fault": fault, }
\ No newline at end of file
Index: fixed_wing_train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\nimport torch\nimport numpy as np\nfrom dynamics.fixed_wing import FixedWing\nfrom qp_control import config\nfrom qp_control.constraints_fw import constraints\nfrom qp_control.datagen import Dataset_with_Grad\nfrom qp_control.trainer_new import Trainer\nfrom qp_control.utils import Utils\nfrom qp_control.NNfuncgrad import CBF, alpha_param, NNController_new\n\nsys.path.insert(1, os.path.abspath('.'))\n\n# import cProfile\n# cProfile.run('foo()')\n\n\nxg = torch.tensor([[100.0,\n                    0.2,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0,\n                    0.0]])\n\nx0 = torch.tensor([[50.0,\n                    0.1,\n                    0.1,\n                    0.4,\n                    0.5,\n                    0.2,\n                    0.1,\n                    0.5,\n                    0.9]])\n\ndt = 0.01\nn_state = 9\nm_control = 4\n\nus_in = input(\"Training with fault (1) or no fault (0):\")\nfault = int(us_in)\n\nnominal_params = {\n    \"m\": 100.0,\n    \"g\": 9.8,\n    \"Ixx\": 100,\n    \"Iyy\": 1000,\n    \"Izz\": 1000,\n    \"Ixz\": 100,\n    \"S\": 100,\n    \"b\": 5,\n    \"bar_c\": 5,\n    \"rho\": 1.2,\n    \"Cd0\": 0.0434,\n    \"Cda\": 0.22,\n    \"Clb\": -0.13,\n    \"Clp\": -0.505,\n    \"Clr\": 0.252,\n    \"Clda\": 0.0855,\n    \"Cldr\": -0.0024,\n    \"Cm0\": 0.135,\n    \"Cma\": -1.50,\n    \"Cmq\": -38.2,\n    \"Cmde\": -0.992,\n    \"Cnb\": 0.0726,\n    \"Cnp\": -0.069,\n    \"Cnr\": -0.0946,\n    \"Cnda\": 0.7,\n    \"Cndr\": -0.0693,\n    \"Cyb\": -0.83,\n    \"Cyp\": 0,\n    \"Cyr\": 0,\n    \"Cydr\": 0.1,\n    \"Cz0\": 0.23,\n    \"Cza\": 4.58,\n    \"Czq\": 0,\n    \"Czde\": 0.1,\n    \"Cx0\": 0,\n    \"Cxq\": 0,\n    \"fault\": fault, }\n\nfault_control_index = 1\n\n\ndef main():\n    dynamics = FixedWing(x=x0, nominal_params=nominal_params, dt=dt, controller_dt=dt)\n    util = Utils(n_state=9, m_control=4, j_const=2, dyn=dynamics, dt=dt, params=nominal_params, fault=fault,\n                 fault_control_index=fault_control_index)\n    nn_controller = NNController_new(n_state=9, m_control=4)\n    cbf = CBF(dynamics, n_state=9, m_control=4)\n    try:\n        if fault == 0:\n            # cbf = CBF(dynamics, n_state=n_state, m_control=m_control)\n            cbf.load_state_dict(torch.load('./data/FW_cbf_NN_weights.pth'))\n            nn_controller.load_state_dict(torch.load('./data/FW_controller_NN_weights.pth'))\n            cbf.eval()\n            nn_controller.eval()\n        else:\n            # cbf = CBF(dynamics, n_state=n_state, m_control=m_control)\n            cbf.load_state_dict(torch.load('./data/FW_cbf_FT_weights.pth'))\n            nn_controller.load_state_dict(torch.load('./data/FW_controller_FT_weights.pth'))\n            cbf.eval()\n            nn_controller.eval()\n    except:\n        print(\"No pre-train data available\")\n\n    alpha = alpha_param(n_state=9)\n    dataset = Dataset_with_Grad(n_state=9, m_control=4, n_pos=1, safe_alpha=0.3, dang_alpha=0.4)\n    trainer = Trainer(nn_controller, cbf, alpha, dataset, n_state=9, m_control=4, j_const=2, dyn=dynamics, n_pos=1,\n                      dt=dt, safe_alpha=0.3, dang_alpha=0.4, action_loss_weight=1, params=nominal_params,\n                      fault=fault,\n                      fault_control_index=fault_control_index)\n    state = x0\n    goal = xg\n    goal = np.array(goal).reshape(1, 9)\n\n    safety_rate = 0.0\n    goal_reached = 0.0\n    um, ul = dynamics.control_limits()\n\n    sm, sl = dynamics.state_limits()\n\n    safe_m, safe_l = dynamics.safe_limits()\n\n    u_nominal = torch.zeros(1, m_control)\n\n    for i in range(config.TRAIN_STEPS):\n        # print(i)\n        if np.mod(i, config.INIT_STATE_UPDATE) == 0 and i > 0:\n            # state = sl.clone().reshape(1, n_state) + torch.randn(1, n_state) * 10\n            state = x0 + torch.randn(1, n_state) * 10\n            state[0, 1] = safe_l[0]\n            state[0, 2] = safe_l[1]\n\n        if np.mod(i, 2 * config.INIT_STATE_UPDATE) == 0 and i > 0:\n            # state = sm.clone().reshape(1, n_state) + torch.randn(1, n_state) * 10\n            state = x0 + torch.randn(1, n_state) * 10\n            state[0, 1] = safe_m[0]\n            state[0, 2] = safe_m[1]\n\n        h, grad_h = cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n\n        for j in range(n_state):\n            if state[0, j] < sl[j] * 0.8:\n                state[0, j] = sl[j].clone()\n            if state[0, j] > sm[j] * 1.5:\n                state[0, j] = sm[j].clone()\n\n        fx = dynamics._f(state, params=nominal_params)\n        gx = dynamics._g(state, params=nominal_params)\n\n        u_n = util.nominal_controller(state=state, goal=goal, u_n=u_nominal, dyn=dynamics, constraints=constraints)\n        u_nominal = util.neural_controller(u_n, fx, gx, h, grad_h, fault_start=0)\n\n        u_nominal = u_nominal.reshape(1, m_control)\n\n        for j in range(m_control):\n            if u_nominal[0, j] < ul[j]:\n                u_nominal[0, j] = ul[j].clone()\n            if u_nominal[0, j] > um[j]:\n                u_nominal[0, j] = um[j].clone()\n\n        u = nn_controller(torch.tensor(state, dtype=torch.float32), torch.tensor(u_nominal, dtype=torch.float32))\n\n        u = torch.squeeze(u.detach())\n\n        if torch.isnan(torch.sum(u)):\n            u = (ul.clone().reshape(m_control) + um.clone().reshape(m_control)) / 2\n\n        if fault == 1:\n            u[fault_control_index] = torch.rand(1)\n\n        for j in range(m_control):\n            if u[j] < ul[j]:\n                u[j] = ul[j].clone()\n            if u[j] > um[j]:\n                u[j] = um[j].clone()\n\n        gxu = torch.matmul(gx, u.reshape(m_control, 1))\n\n        dx = fx.reshape(1, n_state) + gxu.reshape(1, n_state)\n\n        state_next = state + dx * dt\n\n        h, grad_h = cbf.V_with_jacobian(state.reshape(1, n_state, 1))\n\n        dataset.add_data(state, u, u_nominal)\n\n        is_safe = int(util.is_safe(state))\n\n        safety_rate = safety_rate * (1 - 1 / config.POLICY_UPDATE_INTERVAL) + is_safe / config.POLICY_UPDATE_INTERVAL\n\n        state = state_next\n        # done = torch.linalg.norm(state_next.detach().cpu() - goal) < 5\n        done = int(dynamics.goal_mask(state_next))\n\n        if np.mod(i, config.POLICY_UPDATE_INTERVAL) == 0 and i > 0:\n            loss_np, acc_np, loss_h_safe, loss_h_dang, loss_alpha, loss_deriv_safe, loss_deriv_dang, loss_deriv_mid, loss_action, loss_limit = trainer.train_cbf_and_controller()\n            print(\n                'step: {}, train h and u, loss: {:.3f}, safety rate: {:.3f}, goal reached: {:.3f}, acc: {}, '\n                'loss_h_safe: {:.3f}, loss_h_dang: {:.3f}, loss_alpha: {:.3f}, loss_deriv_safe: {:.3f}, '\n                'loss_deriv_dang: {:.3f}, loss_deriv_mid: {:.3f}, loss_action: {:.3f}, loss_limit: {:.3f}'.format(\n                    i, loss_np, safety_rate, goal_reached, acc_np, loss_h_safe, loss_h_dang, loss_alpha,\n                    loss_deriv_safe, loss_deriv_dang, loss_deriv_mid, loss_action, loss_limit))\n            loss_total = loss_np\n\n            if fault == 0:\n                torch.save(cbf.state_dict(), './data/FW_cbf_NN_weights.pth')\n                torch.save(nn_controller.state_dict(), './data/FW_controller_NN_weights.pth')\n                torch.save(alpha.state_dict(), './data/FW_alpha_NN_weights.pth')\n            else:\n                torch.save(cbf.state_dict(), './data/FW_cbf_FT_weights.pth')\n                torch.save(nn_controller.state_dict(), './data/FW_controller_FT_weights.pth')\n                torch.save(alpha.state_dict(), './data/FW_alpha_FT_weights.pth')\n        if done:\n            dist = np.linalg.norm(np.array(state_next, dtype=float) - np.array(goal, dtype=float))\n            goal_reached = goal_reached * (1 - 1e-2) + done * 1e-2\n            state = x0\n\n\nif __name__ == '__main__':\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fixed_wing_train.py b/fixed_wing_train.py
--- a/fixed_wing_train.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/fixed_wing_train.py	(date 1666108541841)
@@ -40,47 +40,12 @@
 n_state = 9
 m_control = 4
 
-us_in = input("Training with fault (1) or no fault (0):")
-fault = int(us_in)
+# us_in = input("Training with fault (1) or no fault (0):")
+# fault = int(us_in)
 
-nominal_params = {
-    "m": 100.0,
-    "g": 9.8,
-    "Ixx": 100,
-    "Iyy": 1000,
-    "Izz": 1000,
-    "Ixz": 100,
-    "S": 100,
-    "b": 5,
-    "bar_c": 5,
-    "rho": 1.2,
-    "Cd0": 0.0434,
-    "Cda": 0.22,
-    "Clb": -0.13,
-    "Clp": -0.505,
-    "Clr": 0.252,
-    "Clda": 0.0855,
-    "Cldr": -0.0024,
-    "Cm0": 0.135,
-    "Cma": -1.50,
-    "Cmq": -38.2,
-    "Cmde": -0.992,
-    "Cnb": 0.0726,
-    "Cnp": -0.069,
-    "Cnr": -0.0946,
-    "Cnda": 0.7,
-    "Cndr": -0.0693,
-    "Cyb": -0.83,
-    "Cyp": 0,
-    "Cyr": 0,
-    "Cydr": 0.1,
-    "Cz0": 0.23,
-    "Cza": 4.58,
-    "Czq": 0,
-    "Czde": 0.1,
-    "Cx0": 0,
-    "Cxq": 0,
-    "fault": fault, }
+nominal_params = config.FIXED_WING_PARAMS
+
+fault = nominal_params["fault"]
 
 fault_control_index = 1
 
@@ -123,7 +88,7 @@
 
     sm, sl = dynamics.state_limits()
 
-    safe_m, safe_l = dynamics.safe_limits()
+    safe_m, safe_l = dynamics.safe_limits(sm, sl)
 
     u_nominal = torch.zeros(1, m_control)
 
@@ -131,22 +96,22 @@
         # print(i)
         if np.mod(i, config.INIT_STATE_UPDATE) == 0 and i > 0:
             # state = sl.clone().reshape(1, n_state) + torch.randn(1, n_state) * 10
-            state = x0 + torch.randn(1, n_state) * 10
-            state[0, 1] = safe_l[0]
-            state[0, 2] = safe_l[1]
+            state = x0 + torch.randn(1, n_state) * 20
+            state[0, 1] = safe_l[1] + 20 * torch.randn(1)
+            state[0, 2] = safe_l[2] + 20 * torch.randn(1)
 
         if np.mod(i, 2 * config.INIT_STATE_UPDATE) == 0 and i > 0:
             # state = sm.clone().reshape(1, n_state) + torch.randn(1, n_state) * 10
-            state = x0 + torch.randn(1, n_state) * 10
-            state[0, 1] = safe_m[0]
-            state[0, 2] = safe_m[1]
+            state = x0 + torch.randn(1, n_state) * 20
+            state[0, 1] = safe_m[1] + 20 * torch.randn(1)
+            state[0, 2] = safe_m[2] + 20 * torch.randn(1)
 
         h, grad_h = cbf.V_with_jacobian(state.reshape(1, n_state, 1))
 
         for j in range(n_state):
-            if state[0, j] < sl[j] * 0.8:
+            if state[0, j] < sl[j] * 0.5:
                 state[0, j] = sl[j].clone()
-            if state[0, j] > sm[j] * 1.5:
+            if state[0, j] > sm[j] * 2:
                 state[0, j] = sm[j].clone()
 
         fx = dynamics._f(state, params=nominal_params)
Index: qp_control/NNfuncgrad.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import itertools\nfrom typing import Tuple, List, Optional\nfrom collections import OrderedDict\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport numpy as np \n\n\nclass CBF(nn.Module):\n\n    def __init__(self, dynamics, n_state, m_control, preprocess_func=None,fault_control_index=1,fault=0):\n        super().__init__()\n        self.n_state = n_state\n        self.fault = fault\n        self.m_control = m_control\n        self.dynamics = dynamics\n        self.preprocess_func = preprocess_func\n        self.fault_control_index = fault_control_index\n\n        self.n_dims_extended = self.n_state\n        self.cbf_hidden_layers = 3\n        self.cbf_hidden_size = 128\n\n        self.V_layers: OrderedDict[str, nn.Module] = OrderedDict()\n\n        self.V_layers[\"input_linear\"] = nn.Linear(\n            self.n_dims_extended, self.cbf_hidden_size\n        )\n        self.V_layers[\"input_activation\"] = nn.Tanh()\n        for i in range(self.cbf_hidden_layers):\n            self.V_layers[f\"layer_{i}_linear\"] = nn.Linear(\n                self.cbf_hidden_size, self.cbf_hidden_size\n            )\n            if i < self.cbf_hidden_layers - 1:\n                self.V_layers[f\"layer_{i}_activation\"] = nn.Tanh()\n        self.V_layers[\"output_linear\"] = nn.Linear(self.cbf_hidden_size, 1)\n        self.V_nn = nn.Sequential(self.V_layers)\n\n\n\n    def forward(self, state):\n        \"\"\"\n        args:\n            state (bs, n_state)\n            obstacle (bs, k_obstacle, n_state)\n        returns:\n            h (bs, k_obstacle)\n        \"\"\"\n        # state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)\n        # state_diff = state\n        \n        # if self.preprocess_func is not None:\n        #     state_diff = self.preprocess_func(state_diff)\n        \n        # x = self.activation(self.conv0(state_diff))\n        # x = self.activation(self.conv1(x))\n        # x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)\n        # x = self.activation(self.conv3(x))\n        # x = self.conv4(x)\n        # h = torch.squeeze(x, dim=1)          # (bs, k_obstacle)\n\n        h, Jh = self.V_with_jacobian(state)\n        # H = torch.tensor(h).reshape(1,1)\n        # JH = torch.tensor(Jh).reshape(1,self.n_state)\n        HJH = torch.hstack((h.reshape(1,1), Jh.reshape(1,self.n_state)))\n        # dh1 = F.conv1d(h,x)\n        return HJH\n\n    def V_with_jacobian(self, x: torch.Tensor):\n        \"\"\"Computes the CLBF value and its Jacobian\n        args:\n            x: bs x self.dynamics_model.n_dims the points at which to evaluate the CLBF\n        returns:\n            V: bs tensor of CLBF values\n            JV: bs x 1 x self.dynamics_model.n_dims Jacobian of each row of V wrt x\n        \"\"\"\n        x_alpha = x[:,1].clone()\n        if self.fault == 0:\n            safe_alpha_m = np.pi / 8.0\n            safe_alpha_l = - np.pi / 80.0\n        else:\n            safe_alpha_m = np.pi / 6.0\n            safe_alpha_l = - np.pi / 60.0\n\n        x_norm = torch.unsqueeze(x, 2)    # (bs, n_state, 1)\n        bs = x_norm.shape[0]\n        x_norm = x_norm.reshape(bs,self.n_state,1)\n\n        # print(x_norm)\n        # print(x_norm.shape)\n        x_norm, x_range = self.normalize(x_norm)\n        \n        # print(x_norm)\n        x_range = x_range.reshape(self.dynamics.n_dims)\n\n        # print(x_range.shape)\n        \n        # print(x_norm.shape)\n        x_norm = x_norm.reshape(bs, self.n_state)\n\n        JV = torch.zeros(\n            (bs, self.dynamics.n_dims, self.dynamics.n_dims)).type_as(x)\n\n        # print(JV.shape)\n\n        for dim in range(self.dynamics.n_dims):\n            JV[:, dim, dim] = 1.0 / x_range[dim].type_as(x)\n        # print(JV.shape)\n       \n        # Now step through each layer in V\n        V = x_norm\n\n        # print(self.V_nn)\n        for layer in self.V_nn:\n            V = layer(V)\n\n            if isinstance(layer, nn.Linear):\n                JV = torch.matmul(layer.weight, JV)\n                # print(\"Linear\")\n                # print(JV.shape)\n            elif isinstance(layer, nn.Tanh):\n                JV = torch.matmul(torch.diag_embed(1 - V ** 2), JV)\n                # print(\"Tanh\")\n                # print(JV.shape)\n            elif isinstance(layer, nn.ReLU):\n                # print(\"Relu\")\n                JV = torch.matmul(torch.diag_embed(torch.sign(V)), JV)\n                # print(JV.shape)\n            # print(V.shape)\n        V_alpha = - 0.5* (x_alpha - (safe_alpha_m + safe_alpha_l) / 2) ** 2 + ((safe_alpha_m - safe_alpha_l) / 2) ** 2\n        V_shape = V.shape\n        V = V + V_alpha.reshape(V_shape)\n\n        JV_alpha = 0.0 * JV.clone()\n        JV_alpha[:,0, 1] = - 0.5* (x_alpha - (safe_alpha_m + safe_alpha_l) / 2).reshape(bs)\n\n        # print(JV.shape)\n        # print(asas)\n        JV = JV + JV_alpha\n        return V, JV\n\n    def normalize(self, x: torch.Tensor, k: float = 1.0):\n        \"\"\"Normalize the state input to [-k, k]\n\n        args:\n            dynamics_model: the dynamics model matching the provided states\n            x: bs x self.dynamics_model.n_dims the points to normalize\n            k: normalize non-angle dimensions to [-k, k]\n        \"\"\"\n        shape_x = x.shape\n\n        # print(shape_x)\n\n        x_max, x_min = self.dynamics.state_limits()\n\n        x_center = (x_max + x_min).type_as(x.clone().detach()) / 2\n        # x_center.to(torch.device('cuda'))\n\n        x_center = x_center.reshape(1,self.n_state,1)\n        # print(x_center.shape)\n        x_range = (x_max - x_min) / 2.0\n        # Scale to get the input between (-k, k), centered at 0\n        x_range = x_range / k\n        # x_range.to(torch.device('cuda'))\n        x_norm = x - x_center # .type_as(x) #.reshape(shape_x)\n        x_range = x_range.reshape(1,self.n_state,1)\n        # print(x_norm.shape)\n        # print(x_center.shape)\n        x_norm = x_norm / x_range.type_as(x)\n        # x_norm = torch.div(x_norm, x_range.type_as(x))\n        # We shouldn't scale or offset any angle dimensions\n        # print(x_norm.shape)\n\n        # Do the normalization\n        return x_norm, x_range\n\n\n\nclass alpha_param(nn.Module):\n\n    def __init__(self, n_state, preprocess_func=None):\n        super().__init__()\n        self.n_state = n_state\n\n        self.preprocess_func = preprocess_func\n\n        self.conv0 = nn.Conv1d(n_state, 64, 1)\n        self.conv1 = nn.Conv1d(64, 128, 1)\n        self.conv2 = nn.Conv1d(128, 128, 1)\n        self.conv3 = nn.Conv1d(128, 128, 1)\n        self.conv4 = nn.Conv1d(128, 1, 1)\n        self.activation = nn.ReLU()\n        self.output_activation = nn.Tanh()\n\n\n    def forward(self, state):\n        \"\"\"\n        args:\n            state (bs, n_state)\n            obstacle (bs, k_obstacle, n_state)\n        returns:\n            h (bs, k_obstacle)\n        \"\"\"\n        state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)\n        state_diff = state\n\n        if self.preprocess_func is not None:\n            state_diff = self.preprocess_func(state_diff)\n        \n        x = self.activation(self.conv0(state_diff))\n        x = self.activation(self.conv1(x))\n        x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)\n        x = self.activation(self.conv3(x))\n        x = self.conv4(x)\n        alpha = torch.squeeze(x, dim=1)          # (bs, k_obstacle)\n        return alpha\n\nclass NNController(nn.Module):\n\n    def __init__(self, n_state, m_control, preprocess_func=None, output_scale=1.0):\n        super().__init__()\n        self.n_state = n_state\n        self.k_obstacle = k_obstacle\n        self.m_control = m_control\n        self.preprocess_func = preprocess_func\n\n        self.conv0 = nn.Conv1d(n_state, 64, 1)\n        self.conv1 = nn.Conv1d(64, 128, 1)\n        self.conv2 = nn.Conv1d(128, 128, 1)\n        self.fc0 = nn.Linear(128 + m_control + n_state, 128)\n        self.fc1 = nn.Linear(128, 64)\n        self.fc2 = nn.Linear(64, m_control)\n        self.activation = nn.ReLU()\n        self.output_activation = nn.Tanh()\n        self.output_scale = output_scale\n\n    def forward(self, state, obstacle, u_nominal, state_error):\n        \"\"\"\n        args:\n            state (bs, n_state)\n            obstacle (bs, k_obstacle, n_state)\n            u_nominal (bs, m_control)\n            state_error (bs, n_state)\n        returns:\n            u (bs, m_control)\n        \"\"\"\n        state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)\n        # print(state)\n        # print(len(state))\n        obstacle = obstacle.permute(0, 2, 1) # (bs, n_state, k_obstacle)\n        state_diff = state - obstacle\n\n        if self.preprocess_func is not None:\n            state_diff = self.preprocess_func(state_diff)\n            state_error = self.preprocess_func(state_error)\n        \n        x = self.activation(self.conv0(state_diff))\n        x = self.activation(self.conv1(x))\n        x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)\n        x, _ = torch.max(x, dim=2)              # (bs, 128)\n        x = torch.cat([x, u_nominal, state_error], dim=1) # (bs, 128 + m_control)\n        x = self.activation(self.fc0(x))\n        x = self.activation(self.fc1(x))\n        x = self.output_activation(self.fc2(x)) * self.output_scale\n        u = x + u_nominal\n        return u\n\n\nclass NNController_new(nn.Module):\n\n    def __init__(self, n_state, m_control, preprocess_func=None, output_scale=1.0):\n        super().__init__()\n        self.n_state = n_state\n        # self.k_obstacle = k_obstacle\n        self.m_control = m_control\n        self.preprocess_func = preprocess_func\n\n        self.conv0 = nn.Conv1d(n_state, 64, 1)\n        self.conv1 = nn.Conv1d(64, 128, 1)\n        self.conv2 = nn.Conv1d(128, 128, 1)\n        self.fc0 = nn.Linear(128 + m_control, 128)\n        self.fc1 = nn.Linear(128, 64)\n        self.fc2 = nn.Linear(64, m_control)\n        self.activation = nn.ReLU()\n        self.output_activation = nn.Tanh()\n        self.output_scale = output_scale\n\n    def forward(self, state, u_nominal):\n        \"\"\"\n        args:\n            state (bs, n_state)\n            obstacle (bs, k_obstacle, n_state)\n            u_nominal (bs, m_control)\n            state_error (bs, n_state)\n        returns:\n            u (bs, m_control)\n        \"\"\"\n        state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)\n        # print(state)\n        # print(len(state))\n        # obstacle = obstacle.permute(0, 2, 1) # (bs, n_state, k_obstacle)\n        # state_diff = state - obstacle\n\n        if self.preprocess_func is not None:\n            state_diff = self.preprocess_func(state)\n            # state_error = self.preprocess_func(state_error)\n        \n        x = self.activation(self.conv0(state))\n        x = self.activation(self.conv1(x))\n        x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)\n        x, _ = torch.max(x, dim=2)              # (bs, 128)\n        x = torch.cat([x, u_nominal], dim=1) # (bs, 128 + m_control)\n        x = self.activation(self.fc0(x))\n        x = self.activation(self.fc1(x))\n        x = self.output_activation(self.fc2(x)) * self.output_scale\n        u = x + u_nominal\n        return u
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/qp_control/NNfuncgrad.py b/qp_control/NNfuncgrad.py
--- a/qp_control/NNfuncgrad.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/qp_control/NNfuncgrad.py	(date 1666034610853)
@@ -7,12 +7,12 @@
 import torch.nn as nn
 import torch.nn.functional as F
 import pytorch_lightning as pl
-import numpy as np 
+import numpy as np
 
 
 class CBF(nn.Module):
 
-    def __init__(self, dynamics, n_state, m_control, preprocess_func=None,fault_control_index=1,fault=0):
+    def __init__(self, dynamics, n_state, m_control, preprocess_func=None, fault_control_index=1, fault=0):
         super().__init__()
         self.n_state = n_state
         self.fault = fault
@@ -40,8 +40,6 @@
         self.V_layers["output_linear"] = nn.Linear(self.cbf_hidden_size, 1)
         self.V_nn = nn.Sequential(self.V_layers)
 
-
-
     def forward(self, state):
         """
         args:
@@ -52,10 +50,10 @@
         """
         # state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)
         # state_diff = state
-        
+
         # if self.preprocess_func is not None:
         #     state_diff = self.preprocess_func(state_diff)
-        
+
         # x = self.activation(self.conv0(state_diff))
         # x = self.activation(self.conv1(x))
         # x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)
@@ -66,7 +64,7 @@
         h, Jh = self.V_with_jacobian(state)
         # H = torch.tensor(h).reshape(1,1)
         # JH = torch.tensor(Jh).reshape(1,self.n_state)
-        HJH = torch.hstack((h.reshape(1,1), Jh.reshape(1,self.n_state)))
+        HJH = torch.hstack((h.reshape(1, 1), Jh.reshape(1, self.n_state)))
         # dh1 = F.conv1d(h,x)
         return HJH
 
@@ -78,7 +76,7 @@
             V: bs tensor of CLBF values
             JV: bs x 1 x self.dynamics_model.n_dims Jacobian of each row of V wrt x
         """
-        x_alpha = x[:,1].clone()
+        x_alpha = x[:, 1].clone()
         if self.fault == 0:
             safe_alpha_m = np.pi / 8.0
             safe_alpha_l = - np.pi / 80.0
@@ -86,19 +84,19 @@
             safe_alpha_m = np.pi / 6.0
             safe_alpha_l = - np.pi / 60.0
 
-        x_norm = torch.unsqueeze(x, 2)    # (bs, n_state, 1)
+        x_norm = torch.unsqueeze(x, 2)  # (bs, n_state, 1)
         bs = x_norm.shape[0]
-        x_norm = x_norm.reshape(bs,self.n_state,1)
+        x_norm = x_norm.reshape(bs, self.n_state, 1)
 
         # print(x_norm)
         # print(x_norm.shape)
         x_norm, x_range = self.normalize(x_norm)
-        
+
         # print(x_norm)
         x_range = x_range.reshape(self.dynamics.n_dims)
 
         # print(x_range.shape)
-        
+
         # print(x_norm.shape)
         x_norm = x_norm.reshape(bs, self.n_state)
 
@@ -110,7 +108,7 @@
         for dim in range(self.dynamics.n_dims):
             JV[:, dim, dim] = 1.0 / x_range[dim].type_as(x)
         # print(JV.shape)
-       
+
         # Now step through each layer in V
         V = x_norm
 
@@ -131,12 +129,12 @@
                 JV = torch.matmul(torch.diag_embed(torch.sign(V)), JV)
                 # print(JV.shape)
             # print(V.shape)
-        V_alpha = - 0.5* (x_alpha - (safe_alpha_m + safe_alpha_l) / 2) ** 2 + ((safe_alpha_m - safe_alpha_l) / 2) ** 2
+        V_alpha = - 0.5 * (x_alpha - (safe_alpha_m + safe_alpha_l) / 2) ** 2 + ((safe_alpha_m - safe_alpha_l) / 2) ** 2
         V_shape = V.shape
         V = V + V_alpha.reshape(V_shape)
 
         JV_alpha = 0.0 * JV.clone()
-        JV_alpha[:,0, 1] = - 0.5* (x_alpha - (safe_alpha_m + safe_alpha_l) / 2).reshape(bs)
+        JV_alpha[:, 0, 1] = - 0.5 * (x_alpha - (safe_alpha_m + safe_alpha_l) / 2).reshape(bs)
 
         # print(JV.shape)
         # print(asas)
@@ -160,14 +158,14 @@
         x_center = (x_max + x_min).type_as(x.clone().detach()) / 2
         # x_center.to(torch.device('cuda'))
 
-        x_center = x_center.reshape(1,self.n_state,1)
+        x_center = x_center.reshape(1, self.n_state, 1)
         # print(x_center.shape)
         x_range = (x_max - x_min) / 2.0
         # Scale to get the input between (-k, k), centered at 0
         x_range = x_range / k
         # x_range.to(torch.device('cuda'))
-        x_norm = x - x_center # .type_as(x) #.reshape(shape_x)
-        x_range = x_range.reshape(1,self.n_state,1)
+        x_norm = x - x_center  # .type_as(x) #.reshape(shape_x)
+        x_range = x_range.reshape(1, self.n_state, 1)
         # print(x_norm.shape)
         # print(x_center.shape)
         x_norm = x_norm / x_range.type_as(x)
@@ -179,7 +177,6 @@
         return x_norm, x_range
 
 
-
 class alpha_param(nn.Module):
 
     def __init__(self, n_state, preprocess_func=None):
@@ -196,7 +193,6 @@
         self.activation = nn.ReLU()
         self.output_activation = nn.Tanh()
 
-
     def forward(self, state):
         """
         args:
@@ -205,20 +201,21 @@
         returns:
             h (bs, k_obstacle)
         """
-        state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)
+        state = torch.unsqueeze(state, 2)  # (bs, n_state, 1)
         state_diff = state
 
         if self.preprocess_func is not None:
             state_diff = self.preprocess_func(state_diff)
-        
+
         x = self.activation(self.conv0(state_diff))
         x = self.activation(self.conv1(x))
-        x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)
+        x = self.activation(self.conv2(x))  # (bs, 128, k_obstacle)
         x = self.activation(self.conv3(x))
         x = self.conv4(x)
-        alpha = torch.squeeze(x, dim=1)          # (bs, k_obstacle)
+        alpha = torch.squeeze(x, dim=1)  # (bs, k_obstacle)
         return alpha
 
+
 class NNController(nn.Module):
 
     def __init__(self, n_state, m_control, preprocess_func=None, output_scale=1.0):
@@ -248,21 +245,21 @@
         returns:
             u (bs, m_control)
         """
-        state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)
+        state = torch.unsqueeze(state, 2)  # (bs, n_state, 1)
         # print(state)
         # print(len(state))
-        obstacle = obstacle.permute(0, 2, 1) # (bs, n_state, k_obstacle)
+        obstacle = obstacle.permute(0, 2, 1)  # (bs, n_state, k_obstacle)
         state_diff = state - obstacle
 
         if self.preprocess_func is not None:
             state_diff = self.preprocess_func(state_diff)
             state_error = self.preprocess_func(state_error)
-        
+
         x = self.activation(self.conv0(state_diff))
         x = self.activation(self.conv1(x))
-        x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)
-        x, _ = torch.max(x, dim=2)              # (bs, 128)
-        x = torch.cat([x, u_nominal, state_error], dim=1) # (bs, 128 + m_control)
+        x = self.activation(self.conv2(x))  # (bs, 128, k_obstacle)
+        x, _ = torch.max(x, dim=2)  # (bs, 128)
+        x = torch.cat([x, u_nominal, state_error], dim=1)  # (bs, 128 + m_control)
         x = self.activation(self.fc0(x))
         x = self.activation(self.fc1(x))
         x = self.output_activation(self.fc2(x)) * self.output_scale
@@ -299,7 +296,7 @@
         returns:
             u (bs, m_control)
         """
-        state = torch.unsqueeze(state, 2)    # (bs, n_state, 1)
+        state = torch.unsqueeze(state, 2)  # (bs, n_state, 1)
         # print(state)
         # print(len(state))
         # obstacle = obstacle.permute(0, 2, 1) # (bs, n_state, k_obstacle)
@@ -308,14 +305,14 @@
         if self.preprocess_func is not None:
             state_diff = self.preprocess_func(state)
             # state_error = self.preprocess_func(state_error)
-        
+
         x = self.activation(self.conv0(state))
         x = self.activation(self.conv1(x))
-        x = self.activation(self.conv2(x))   # (bs, 128, k_obstacle)
-        x, _ = torch.max(x, dim=2)              # (bs, 128)
-        x = torch.cat([x, u_nominal], dim=1) # (bs, 128 + m_control)
+        x = self.activation(self.conv2(x))  # (bs, 128, k_obstacle)
+        x, _ = torch.max(x, dim=2)  # (bs, 128)
+        x = torch.cat([x, u_nominal], dim=1)  # (bs, 128 + m_control)
         x = self.activation(self.fc0(x))
         x = self.activation(self.fc1(x))
         x = self.output_activation(self.fc2(x)) * self.output_scale
         u = x + u_nominal
-        return u
\ No newline at end of file
+        return u
Index: .idea/other.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/other.xml b/.idea/other.xml
new file mode 100644
--- /dev/null	(date 1665756392834)
+++ b/.idea/other.xml	(date 1665756392834)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="PySciProjectComponent">
+    <option name="PY_SCI_VIEW_SUGGESTED" value="true" />
+  </component>
+</project>
\ No newline at end of file
Index: cbf_eval.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\nsys.path.insert(1, os.path.abspath('.'))\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport numpy as np\nimport torch\nfrom qp_control.NNfuncgrad_CF import CBF\nfrom dynamics.Crazyflie import CrazyFlies\nfrom qp_control.utils_crazy import Utils\n\n\nn_state = 12\nm_control = 4\n\nfault = int(input(\"Fault (1) or pre-fault (0):\"))\n\nfault_control_index = 1\n\ndt = 0.01\nN = 10000\n\nnominal_params = {\n    \"m\": 0.0299,\n    \"Ixx\": 1.395 * 10**(-5),\n    \"Iyy\": 1.395 * 10**(-5),\n    \"Izz\": 2.173 * 10**(-5),\n    \"CT\": 3.1582 * 10**(-10),\n    \"CD\": 7.9379 * 10**(-12),\n    \"d\": 0.03973,\n    \"fault\": fault,}\n\nstate0 = torch.tensor([[2.0,\n                    2.0,\n                    3.5,\n                    0.0,\n                    0.0,\n                    0.0,\n                    np.pi / 20.0,\n                    np.pi / 20.0,\n                    np.pi / 20.0,\n                    np.pi / 40.0,\n                    np.pi / 40.0,\n                    np.pi / 40.0]])\n\ndynamics = CrazyFlies(x=state0, nominal_params=nominal_params, dt=dt, controller_dt=dt)\nutil = Utils(n_state=12, m_control=4, j_const=2, dyn=dynamics, dt=dt, params=nominal_params, fault=fault,\n                 fault_control_index=fault_control_index)\n\nsu, sl = dynamics.state_limits()\n\nif fault == 0:\n    cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault = 0, fault_control_index = 1)\n    cbf.load_state_dict(torch.load('./data/CF_cbf_NN_weights.pth'))\n    cbf.eval()\nelse:\n    cbf = CBF(dynamics, n_state=n_state, m_control=m_control, fault = 1, fault_control_index = 1)\n    cbf.load_state_dict(torch.load('./data/CF_cbf_FT_weights.pth'))\n    cbf.eval()\n\nsafety_rate = 0.0\ncorrect_h = 0.0\n\n\nstate = torch.tensor([]).reshape(0,n_state) #torch.zeros(N,n_state).reshape(N,n_state)\nfor j in range(N):\n    state_N = state0.reshape(1,n_state) + 1 * torch.randn(1, n_state)\n    state = torch.vstack((state,state_N))\n\nstate = state.reshape(N,n_state,1)\n\nh, _  = cbf.V_with_jacobian(state)\n\nsafety_rate = torch.sum(util.is_safe(state)) / N\n\nun_safety_rate = torch.sum(torch.logical_not(util.is_safe(state))) / N\n\ncorrect_h_safe = torch.sum(util.is_safe(state).reshape(1, N) * (h >= 0).reshape(1, N)) / N\ncorrect_h_un_safe = torch.sum(util.is_unsafe(state).reshape(1, N) * (h < 0).reshape(1, N)) / N\n\nprint(safety_rate)\nprint(un_safety_rate)\nprint(correct_h_safe / (safety_rate + 1e-5))\nprint(correct_h_un_safe / (un_safety_rate + 1e-5))\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cbf_eval.py b/cbf_eval.py
--- a/cbf_eval.py	(revision 8a291c6a6e409b06976bafdb9a0dbd7c3e2b54b4)
+++ b/cbf_eval.py	(date 1666027694539)
@@ -63,7 +63,8 @@
 correct_h = 0.0
 
 
-state = torch.tensor([]).reshape(0,n_state) #torch.zeros(N,n_state).reshape(N,n_state)
+state = torch.tensor([]).reshape(0,n_state)  # torch.zeros(N,n_state).reshape(N,n_state)
+
 for j in range(N):
     state_N = state0.reshape(1,n_state) + 1 * torch.randn(1, n_state)
     state = torch.vstack((state,state_N))
